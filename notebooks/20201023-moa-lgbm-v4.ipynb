{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thanks for:\n\nhttps://www.kaggle.com/sishihara/moa-lgbm-benchmark#Preprocessing\n\nhttps://www.kaggle.com/ttahara/osic-baseline-lgbm-with-custom-metric\n\nhttps://zenn.dev/fkubota/articles/2b8d46b11c178ac2fa2d\n\nhttps://qiita.com/ryouta0506/items/619d9ac0d80f8c0aed92\n\nhttps://github.com/nejumi/tools_for_kaggle/blob/master/semi_supervised_learner.py\n\nhttps://upura.hatenablog.com/entry/2019/03/03/233534\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Version = \"v1\" # starter model\n# Version = \"v2\" # Add debug mode and minor modifications\n# Version = \"v3\"  # minor modifications, DEBUG=True:CV:0.01672, False:CV:0.01636\n\nVersion = \"v4\" # Use anotated data, ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEBUG = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nimport random\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import NearestNeighbors\n\nfrom tqdm.notebook import tqdm\nimport torch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"lightgbm Version: \", lgb.__version__)\nprint(\"numpy Version: \", np.__version__)\nprint(\"pandas Version: \", pd.__version__)\nprint(\"imblearn Version: \", imblearn.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_logger(filename='log'):\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.{Version}.moa.lgbm.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = get_logger()\n\n\ndef seed_everything(seed=777):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEBUG:\n    N_FOLD = 2\n    Num_boost_round=1000\n    Early_stopping_rounds=10\n    Learning_rate = 0.03\nelse:\n    N_FOLD = 4\n    Num_boost_round=10000\n    Early_stopping_rounds=50\n    Learning_rate = 0.01\n\nSEED = 42\nseed_everything(seed=SEED)\n\nThreshold = 0.6\nMax_depth = 7","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Loading"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/lish-moa/train_features.csv\")\ntest = pd.read_csv(\"../input/lish-moa/test_features.csv\")\ntrain_targets_scored = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ntrain_targets_nonscored = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\nsub = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_encoding(train: pd.DataFrame, test: pd.DataFrame, encode_cols):\n    n_train = len(train)\n    train = pd.concat([train, test], sort=False).reset_index(drop=True)\n    for f in encode_cols:\n        try:\n            lbl = preprocessing.LabelEncoder()\n            train[f] = lbl.fit_transform(list(train[f].values))\n        except:\n            print(f)\n    test = train[n_train:].reset_index(drop=True)\n    train = train[:n_train]\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annot = pd.read_csv(\"../input/moa-annot-data/20201016_moa_sig_list.csv\")\nannot.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annot_sig = annot.sig_id.tolist()\nprint(annot_sig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target = pd.concat([train_targets_scored, train_targets_nonscored], axis=1)\ntrain_target.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_target(target_col, annot_sig):\n    if target_col in annot_sig:\n        t_cols = []\n        for t_col in list(annot[annot.sig_id == target_col].iloc[0]):\n            if t_col is not np.nan:\n                t_cols.append(t_col)\n                target = train_target[t_cols]\n                target = target.sum(axis=1)\n                #1以上であれば1に置換\n                target = target.where(target < 1, 1)\n    else:\n        target = train_targets_scored[target_col]\n    \n    return target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#===========================================================\n# model\n#===========================================================\ndef run_lgbm(target_col: str):\n    \n    X_train = train.drop([\"sig_id\"], axis=1)\n    y_train = get_target(target_col, annot_sig)\n    X_test = test.drop([\"sig_id\"], axis=1)\n\n    y_preds = []\n    models = []\n    oof_train = np.zeros((len(X_train),))\n    score = 0\n\n    for fold_, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n        X_tr = X_train.loc[train_index, :]\n        X_val = X_train.loc[valid_index, :]\n        y_tr = y_train[train_index]\n        y_val = y_train[valid_index]\n\n        lgb_train = lgb.Dataset(X_tr,\n                                y_tr,\n                                categorical_feature=categorical_cols)\n\n        lgb_eval = lgb.Dataset(X_val,\n                               y_val,\n                               reference=lgb_train,\n                               categorical_feature=categorical_cols)\n        \n        logger.info(f\"================================= fold {fold_+1}/{cv.get_n_splits()} {target_col}=================================\")\n        \n\n        model = lgb.train(params,\n                          lgb_train,\n                          valid_sets=[lgb_train, lgb_eval],\n                          verbose_eval=100,\n                          num_boost_round=Num_boost_round,\n                          early_stopping_rounds=Early_stopping_rounds)\n\n        oof_train[valid_index] = model.predict(X_val,\n                                               num_iteration=model.best_iteration)\n        y_pred = model.predict(X_test,\n                               num_iteration=model.best_iteration)\n\n        y_preds.append(y_pred)\n        models.append(model)\n        \n    score = log_loss(y_train, oof_train)\n    \n    logger.info(f\"{target_col} logloss: {score}\")\n    logger.info(f\"=========================================================================================\")\n\n    return sum(y_preds) / len(y_preds), score, models, ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_feature_importance(feature_importance_df, num=100):\n    cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n            .groupby(\"Feature\")\n            .mean()\n            .sort_values(by=\"importance\", ascending=False)[:num].index)\n    best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\n    plt.figure(figsize=(8, 30))\n    sns.barplot(x=\"importance\", \n                y=\"Feature\", \n                data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('Features importance (averaged)')\n    plt.tight_layout()\n    plt.savefig(f\"./feature_importance_{Version}.png\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\nWe have to convert some categorical features into numbers in train and test. We can identify categorical features by `pd.DataFrame.select_dtypes`."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes(include=['object']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = label_encoding(train, test, ['cp_type', 'cp_dose'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\n\nparams = {\n    'objective': 'binary',\n    'learning_rate': Learning_rate,\n    'num_threads': 2,\n    'verbose': -1,\n    'max_depth': Max_depth,\n    'num_leaves': int((Max_depth**2)*0.7),\n    'feature_fraction':0.7, # randomly select part of features on each iteration\n    'lambda_l1':0.1,\n    'lambda_l2':0.1\n\n}\n\ncategorical_cols = ['cp_type', 'cp_dose']\noof = train_targets_scored.copy()\nfeature_importance_df = pd.DataFrame()\nmodels = []\nscores = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for target_col in tqdm(train_targets_scored.columns[1:]):\n    _preds, _score, models = run_lgbm(target_col)\n\n    sub[target_col] = _preds\n    scores.append(_score)\n        \n    for model in models:\n        _importance_df = pd.DataFrame()\n        _importance_df[\"Feature\"] = train.columns[1:]\n        _importance_df[\"importance\"] = model.feature_importance(importance_type='gain')\n        feature_importance_df = pd.concat([feature_importance_df, _importance_df], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"CV:{np.mean(scores)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_feature_importance(feature_importance_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}