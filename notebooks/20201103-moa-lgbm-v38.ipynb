{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thanks for:\n* https://www.kaggle.com/sishihara/moa-lgbm-benchmark#Preprocessing\n\n* https://www.kaggle.com/ttahara/osic-baseline-lgbm-with-custom-metric\n\n* https://zenn.dev/fkubota/articles/2b8d46b11c178ac2fa2d\n\n* https://qiita.com/ryouta0506/items/619d9ac0d80f8c0aed92\n\n* https://github.com/nejumi/tools_for_kaggle/blob/master/semi_supervised_learner.py\n\n* https://upura.hatenablog.com/entry/2019/03/03/233534\n\n* https://pompom168.hatenablog.com/entry/2019/07/22/113433\n\n* https://www.kaggle.com/c/lish-moa/discussion/193878\n\n* https://tsumit.hatenablog.com/entry/2020/06/20/044835\n\n* https://www.kaggle.com/kushal1506/moa-pytorch-feature-engineering-0-01846\n\n* https://www.kaggle.com/c/lish-moa/discussion/195195\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Version = \"v1\" # starter model\n# Version = \"v2\" # Compare treat Vs. ctrl and minor modifications, StratifiedKFold\n# Version = \"v3\" # Add debug mode and minor modifications\n# Version = \"v4\" # Clipping a control with an outlier(25-75)\n# Version = \"v5\" # Clipping a control with an outlier(20-80)\n# Version = \"v6\" # under sampling 500 → oversamplling 500, lipping a control with an outlier(10-90)\n# Version = \"v7\" # Use anotated data, under sampling 500 → oversamplling 500, clipping a control with an outlier(10-90)\n# Version = \"v8\" # pseudo labeling (thresholds:0.5), timeout\n# Version = \"v9\" # pseudo labeling (thresholds:0.6), timeout\n# Version = \"v10\" # pseudo labeling (thresholds:0.6), ReduceCol: Kolmogorov-Smirnov, PCA(whiten)&UMAP\n# Version = \"v11\" # pseudo labeling (thresholds:0.6), ReduceCol: Kolmogorov-Smirnov, PCA(whiten)&UMAP, lgbm parames adjust\n# Version = \"v12\" # Feature engineering based on feature importance\n# Version = \"v13\" # Calibration, SMOTE(k_neighbors=5→1)\n# Version = \"v14\" # Removed the Calibration, SMOTE(k_neighbors=1), pseudo labeling (thresholds:0.7)\n# Version = \"v15\" # Updata anotated data\n# Version = \"v16\" # Remove noisy label(confidence: 0.5)\n# Version = \"v17\" # Modifications with remove noisy label func, Calibration, confidence = y_prob.probability.max()*0.3\n# Version = \"v18\" # SMOTE(k_neighbors=1→2), confidence = y_prob.probability.max()*0.2\n# Version = \"v19\" # SMOTE(k_neighbors=2→3),\n# Version = \"v20\" # Modifications with confidence, Removed the Calibration, SMOTE(k_neighbors=2), \n# Version = \"v21\" # DEBUG = False\n# Version = \"v22\" # minor modifications\n# Version = \"v23\" # TOP100→PCA→UMAP(n_components=3)\n# Version = \"v24\" # TOP100→PCA→UMAP(n_components=10), UMAP(n_components=2→3)\n# Version = \"v25\" # Feature engineering based on Feature importance\n# Version = \"v26\" # Modify pseudo labeling func to exclude low confidence pseudo labels in the TEST data.\n# Version = \"v27\" # LGBMClassifie:clf.predict→clf.predict_proba\n# Version = \"v28\" # Calibration (No calbration:CV:0.06542)\n# Version = \"v29\" # Remove Calibration, is_unbalance': True, SMOTE(k_neighbors=2→3), Modify pseudo labeling func to include low confidence pseudo labels in the TEST data, target_rate *= 1.2\n# Version = \"v30\" # drop_duplicates(keep=\"last\")\n# Version = \"v31\" # target_rate *= 1.1, if Threshold <= 0.2: break, if sum(p_label)*1.5 >= check: break, if sum(p_label) <= check*1.5: break\n# Version = \"v32\" # y_prob.probability.quantile(0.3), if Threshold >= 0.95: break\n# Version = \"v33\" # RankGauss, Scaled by category, SMOTE(k_neighbors=2),\n# Version = \"v34\" # RankGauss apply c-columns, remove TOP100, Add f_diff = lambda x: x - med, Create features\n# Version = \"v35\" # f_div = lambda x: ((x+d)*10 / (abs(med)+d))**2, f_diff = lambda x: ((x-med)*10)**2, select features\n# Version = \"v36\" # Add feature importance func\n# Version = \"v37\" # Remove RankGauss for gene expression, fix feature importance func\n\nVersion = \"v38\" # Add MultiLabel Stratification func, fix index of data before split with \"data = data.sort_index(axis='index')\"\"\n\n# Feature engineering based on Feature importance with v36 notebook","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEBUG = True","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Library"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\nfrom logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nimport random\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tqdm.notebook import tqdm\nimport torch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"lightgbm Version: \", lgb.__version__)\nprint(\"imblearn Version: \", imblearn.__version__)\nprint(\"numpy Version: \", np.__version__)\nprint(\"pandas Version: \", pd.__version__)","execution_count":3,"outputs":[{"output_type":"stream","text":"lightgbm Version:  2.3.1\nimblearn Version:  0.7.0\nnumpy Version:  1.18.5\npandas Version:  1.1.3\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_logger(filename='log'):\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.{Version}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = get_logger()\n\ndef seed_everything(seed=777):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEBUG:\n    N_FOLD = 2\n    Num_boost_round=1000\n    Early_stopping_rounds=10\n    Learning_rate = 0.03\nelse:\n    N_FOLD = 4\n    Num_boost_round=10000\n    Early_stopping_rounds=30\n    Learning_rate = 0.01\n\nSEED = 42\nseed_everything(seed=SEED)\n\nMax_depth = 7","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Loading"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/lish-moa/train_features.csv\")\ntest = pd.read_csv(\"../input/lish-moa/test_features.csv\")\ntrain_targets_scored = pd.read_csv(\"../input/lish-moa/train_targets_scored.csv\")\ntrain_targets_nonscored = pd.read_csv(\"../input/lish-moa/train_targets_nonscored.csv\")\nsub = pd.read_csv(\"../input/lish-moa/sample_submission.csv\")\n\n# New data file available from 3th November\ndrug = pd.read_csv('../input/lish-moa/train_drug.csv')","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Targets = train_targets_scored.columns[1:]\nScored = train_targets_scored.merge(drug, on='sig_id', how='left')\nScored","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"             sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n0      id_000644bb2                            0                       0   \n1      id_000779bfc                            0                       0   \n2      id_000a6266a                            0                       0   \n3      id_0015fd391                            0                       0   \n4      id_001626bd3                            0                       0   \n...             ...                          ...                     ...   \n23809  id_fffb1ceed                            0                       0   \n23810  id_fffb70c0c                            0                       0   \n23811  id_fffc1c3f4                            0                       0   \n23812  id_fffcb9e7c                            0                       0   \n23813  id_ffffdd77b                            0                       0   \n\n       acat_inhibitor  acetylcholine_receptor_agonist  \\\n0                   0                               0   \n1                   0                               0   \n2                   0                               0   \n3                   0                               0   \n4                   0                               0   \n...               ...                             ...   \n23809               0                               0   \n23810               0                               0   \n23811               0                               0   \n23812               0                               0   \n23813               0                               0   \n\n       acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n0                                      0                               0   \n1                                      0                               0   \n2                                      0                               0   \n3                                      0                               0   \n4                                      0                               0   \n...                                  ...                             ...   \n23809                                  0                               0   \n23810                                  0                               0   \n23811                                  0                               0   \n23812                                  0                               0   \n23813                                  0                               0   \n\n       adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n0                               0                              0   \n1                               0                              0   \n2                               0                              0   \n3                               0                              0   \n4                               0                              0   \n...                           ...                            ...   \n23809                           0                              0   \n23810                           0                              0   \n23811                           0                              0   \n23812                           0                              0   \n23813                           0                              0   \n\n       adenylyl_cyclase_activator  ...  trpv_agonist  trpv_antagonist  \\\n0                               0  ...             0                0   \n1                               0  ...             0                0   \n2                               0  ...             0                0   \n3                               0  ...             0                0   \n4                               0  ...             0                0   \n...                           ...  ...           ...              ...   \n23809                           0  ...             0                0   \n23810                           0  ...             0                0   \n23811                           0  ...             0                0   \n23812                           0  ...             0                0   \n23813                           0  ...             0                0   \n\n       tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n0                      0                          0   \n1                      0                          0   \n2                      0                          0   \n3                      0                          0   \n4                      0                          0   \n...                  ...                        ...   \n23809                  0                          0   \n23810                  0                          0   \n23811                  0                          0   \n23812                  0                          0   \n23813                  0                          0   \n\n       ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n0                                          0                0          0   \n1                                          0                0          0   \n2                                          0                0          0   \n3                                          0                0          0   \n4                                          0                0          0   \n...                                      ...              ...        ...   \n23809                                      0                0          0   \n23810                                      0                0          0   \n23811                                      0                0          0   \n23812                                      0                0          0   \n23813                                      0                0          0   \n\n       vitamin_d_receptor_agonist  wnt_inhibitor    drug_id  \n0                               0              0  b68db1d53  \n1                               0              0  df89a8e5a  \n2                               0              0  18bb41b2c  \n3                               0              0  8c7f86626  \n4                               0              0  7cbed3131  \n...                           ...            ...        ...  \n23809                           0              0  df1d0a5a1  \n23810                           0              0  ecf3b6b74  \n23811                           0              0  cacb2b860  \n23812                           0              0  8b87a7a83  \n23813                           0              0  972f41291  \n\n[23814 rows x 208 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>5-alpha_reductase_inhibitor</th>\n      <th>11-beta-hsd1_inhibitor</th>\n      <th>acat_inhibitor</th>\n      <th>acetylcholine_receptor_agonist</th>\n      <th>acetylcholine_receptor_antagonist</th>\n      <th>acetylcholinesterase_inhibitor</th>\n      <th>adenosine_receptor_agonist</th>\n      <th>adenosine_receptor_antagonist</th>\n      <th>adenylyl_cyclase_activator</th>\n      <th>...</th>\n      <th>trpv_agonist</th>\n      <th>trpv_antagonist</th>\n      <th>tubulin_inhibitor</th>\n      <th>tyrosine_kinase_inhibitor</th>\n      <th>ubiquitin_specific_protease_inhibitor</th>\n      <th>vegfr_inhibitor</th>\n      <th>vitamin_b</th>\n      <th>vitamin_d_receptor_agonist</th>\n      <th>wnt_inhibitor</th>\n      <th>drug_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>b68db1d53</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_000779bfc</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>df89a8e5a</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_000a6266a</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>18bb41b2c</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_0015fd391</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8c7f86626</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_001626bd3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7cbed3131</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23809</th>\n      <td>id_fffb1ceed</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>df1d0a5a1</td>\n    </tr>\n    <tr>\n      <th>23810</th>\n      <td>id_fffb70c0c</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ecf3b6b74</td>\n    </tr>\n    <tr>\n      <th>23811</th>\n      <td>id_fffc1c3f4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>cacb2b860</td>\n    </tr>\n    <tr>\n      <th>23812</th>\n      <td>id_fffcb9e7c</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8b87a7a83</td>\n    </tr>\n    <tr>\n      <th>23813</th>\n      <td>id_ffffdd77b</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>972f41291</td>\n    </tr>\n  </tbody>\n</table>\n<p>23814 rows × 208 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_encoding(train: pd.DataFrame, test: pd.DataFrame, encode_cols):\n    n_train = len(train)\n    train = pd.concat([train, test], sort=False).reset_index(drop=True)\n    \n    for f in encode_cols:\n        try:\n            lbl = preprocessing.LabelEncoder()\n            train[f] = lbl.fit_transform(list(train[f].values))\n        except:\n            print(f)\n    test = train[n_train:].reset_index(drop=True)\n    train = train[:n_train]\n    \n    return train, test","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Manual annotation by myself\nannot = pd.read_csv(\"../input/moa-annot-data/20201024_moa_sig_list.v2.csv\")\nannot","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"                                        sig_id  \\\n0                   adenylyl_cyclase_activator   \n1             aldehyde_dehydrogenase_inhibitor   \n2                               antiarrhythmic   \n3                               anticonvulsant   \n4                                   antifungal   \n5                                antihistamine   \n6   atp-sensitive_potassium_channel_antagonist   \n7       bacterial_membrane_integrity_inhibitor   \n8                        calcineurin_inhibitor   \n9       catechol_o_methyltransferase_inhibitor   \n10                               cdk_inhibitor   \n11                coagulation_factor_inhibitor   \n12                          elastase_inhibitor   \n13                             erbb2_inhibitor   \n14                  nicotinic_receptor_agonist   \n15           nitric_oxide_production_inhibitor   \n16               protein_phosphatase_inhibitor   \n17                sphingosine_receptor_agonist   \n18                                     steroid   \n19       ubiquitin_specific_protease_inhibitor   \n\n                                   nonscored1  \\\n0                  adenylyl_cyclase_inhibitor   \n1             alcohol_dehydrogenase_inhibitor   \n2                       na_k-atpase_inhibitor   \n3         gaba_gated_chloride_channel_blocker   \n4                 fungal_ergosterol_inhibitor   \n5                 histamine_release_inhibitor   \n6   atp-sensitive_potassium_channel_inhibitor   \n7       bacterial_protein_synthesis_inhibitor   \n8                            t_cell_inhibitor   \n9                   dopamine_release_enhancer   \n10                         cyclin_d_inhibitor   \n11                       vitamin_k_antagonist   \n12               leukocyte_elastase_inhibitor   \n13          protein_tyrosine_kinase_inhibitor   \n14             acetylcholine_release_enhancer   \n15                                vasodilator   \n16             tyrosine_phosphatase_inhibitor   \n17   sphingosine_1_phosphate_receptor_agonist   \n18                          anti-inflammatory   \n19     ubiquitin-conjugating_enzyme_inhibitor   \n\n                                 nonscored2  \n0                                       NaN  \n1                                       NaN  \n2                 potassium_channel_blocker  \n3                                       NaN  \n4   fungal_lanosterol_demethylase_inhibitor  \n5                                       NaN  \n6                                       NaN  \n7                                       NaN  \n8                                       NaN  \n9                                       NaN  \n10                                      NaN  \n11                                      NaN  \n12                                      NaN  \n13                                      NaN  \n14                                      NaN  \n15                                      NaN  \n16                                      NaN  \n17                                      NaN  \n18                                      NaN  \n19                                      NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>nonscored1</th>\n      <th>nonscored2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>adenylyl_cyclase_activator</td>\n      <td>adenylyl_cyclase_inhibitor</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>aldehyde_dehydrogenase_inhibitor</td>\n      <td>alcohol_dehydrogenase_inhibitor</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>antiarrhythmic</td>\n      <td>na_k-atpase_inhibitor</td>\n      <td>potassium_channel_blocker</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>anticonvulsant</td>\n      <td>gaba_gated_chloride_channel_blocker</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>antifungal</td>\n      <td>fungal_ergosterol_inhibitor</td>\n      <td>fungal_lanosterol_demethylase_inhibitor</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>antihistamine</td>\n      <td>histamine_release_inhibitor</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>atp-sensitive_potassium_channel_antagonist</td>\n      <td>atp-sensitive_potassium_channel_inhibitor</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>bacterial_membrane_integrity_inhibitor</td>\n      <td>bacterial_protein_synthesis_inhibitor</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>calcineurin_inhibitor</td>\n      <td>t_cell_inhibitor</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>catechol_o_methyltransferase_inhibitor</td>\n      <td>dopamine_release_enhancer</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>cdk_inhibitor</td>\n      <td>cyclin_d_inhibitor</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>coagulation_factor_inhibitor</td>\n      <td>vitamin_k_antagonist</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>elastase_inhibitor</td>\n      <td>leukocyte_elastase_inhibitor</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>erbb2_inhibitor</td>\n      <td>protein_tyrosine_kinase_inhibitor</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>nicotinic_receptor_agonist</td>\n      <td>acetylcholine_release_enhancer</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>nitric_oxide_production_inhibitor</td>\n      <td>vasodilator</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>protein_phosphatase_inhibitor</td>\n      <td>tyrosine_phosphatase_inhibitor</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>sphingosine_receptor_agonist</td>\n      <td>sphingosine_1_phosphate_receptor_agonist</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>steroid</td>\n      <td>anti-inflammatory</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>ubiquitin_specific_protease_inhibitor</td>\n      <td>ubiquitin-conjugating_enzyme_inhibitor</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"annot_sig = []\nannot_sig = annot.sig_id.tolist()\nprint(annot_sig)","execution_count":10,"outputs":[{"output_type":"stream","text":"['adenylyl_cyclase_activator', 'aldehyde_dehydrogenase_inhibitor', 'antiarrhythmic', 'anticonvulsant', 'antifungal', 'antihistamine', 'atp-sensitive_potassium_channel_antagonist', 'bacterial_membrane_integrity_inhibitor', 'calcineurin_inhibitor', 'catechol_o_methyltransferase_inhibitor', 'cdk_inhibitor', 'coagulation_factor_inhibitor', 'elastase_inhibitor', 'erbb2_inhibitor', 'nicotinic_receptor_agonist', 'nitric_oxide_production_inhibitor', 'protein_phosphatase_inhibitor', 'sphingosine_receptor_agonist', 'steroid', 'ubiquitin_specific_protease_inhibitor']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target = pd.concat([train_targets_scored, train_targets_nonscored], axis=1)\ntrain_target.head() ","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n0  id_000644bb2                            0                       0   \n1  id_000779bfc                            0                       0   \n2  id_000a6266a                            0                       0   \n3  id_0015fd391                            0                       0   \n4  id_001626bd3                            0                       0   \n\n   acat_inhibitor  acetylcholine_receptor_agonist  \\\n0               0                               0   \n1               0                               0   \n2               0                               0   \n3               0                               0   \n4               0                               0   \n\n   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n0                                  0                               0   \n1                                  0                               0   \n2                                  0                               0   \n3                                  0                               0   \n4                                  0                               0   \n\n   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n0                           0                              0   \n1                           0                              0   \n2                           0                              0   \n3                           0                              0   \n4                           0                              0   \n\n   adenylyl_cyclase_activator  ...  ve-cadherin_antagonist  \\\n0                           0  ...                       0   \n1                           0  ...                       0   \n2                           0  ...                       0   \n3                           0  ...                       0   \n4                           0  ...                       0   \n\n   vesicular_monoamine_transporter_inhibitor  vitamin_k_antagonist  \\\n0                                          0                     0   \n1                                          0                     0   \n2                                          0                     0   \n3                                          0                     0   \n4                                          0                     0   \n\n   voltage-gated_calcium_channel_ligand  \\\n0                                     0   \n1                                     0   \n2                                     0   \n3                                     0   \n4                                     0   \n\n   voltage-gated_potassium_channel_activator  \\\n0                                          0   \n1                                          0   \n2                                          0   \n3                                          0   \n4                                          0   \n\n   voltage-gated_sodium_channel_blocker  wdr5_mll_interaction_inhibitor  \\\n0                                     0                               0   \n1                                     0                               0   \n2                                     0                               0   \n3                                     0                               0   \n4                                     0                               0   \n\n   wnt_agonist  xanthine_oxidase_inhibitor  xiap_inhibitor  \n0            0                           0               0  \n1            0                           0               0  \n2            0                           0               0  \n3            0                           0               0  \n4            0                           0               0  \n\n[5 rows x 610 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>5-alpha_reductase_inhibitor</th>\n      <th>11-beta-hsd1_inhibitor</th>\n      <th>acat_inhibitor</th>\n      <th>acetylcholine_receptor_agonist</th>\n      <th>acetylcholine_receptor_antagonist</th>\n      <th>acetylcholinesterase_inhibitor</th>\n      <th>adenosine_receptor_agonist</th>\n      <th>adenosine_receptor_antagonist</th>\n      <th>adenylyl_cyclase_activator</th>\n      <th>...</th>\n      <th>ve-cadherin_antagonist</th>\n      <th>vesicular_monoamine_transporter_inhibitor</th>\n      <th>vitamin_k_antagonist</th>\n      <th>voltage-gated_calcium_channel_ligand</th>\n      <th>voltage-gated_potassium_channel_activator</th>\n      <th>voltage-gated_sodium_channel_blocker</th>\n      <th>wdr5_mll_interaction_inhibitor</th>\n      <th>wnt_agonist</th>\n      <th>xanthine_oxidase_inhibitor</th>\n      <th>xiap_inhibitor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_000779bfc</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_000a6266a</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_0015fd391</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_001626bd3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 610 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Training Utils"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_target(target_col, annot_sig):\n    if target_col in annot_sig:\n        t_cols = []\n        for t_col in list(annot[annot.sig_id == target_col].iloc[0]):\n            if t_col is not np.nan:\n                t_cols.append(t_col)\n                target = train_target[t_cols]\n                target = target.sum(axis=1)\n                #1 or more, replace it with 1.\n                target = target.where(target < 1, 1)\n    else:\n        target = train_targets_scored[target_col]\n    \n    return target","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Multi_Stratification(df, target_col, target):\n    \n    _df = df.copy() \n    sig_id_lst = [list(Scored.sig_id[Scored.drug_id == id_].sample())[0] for id_ in Scored.drug_id.unique()]\n    \n    # Remove sig_id wih target \n    del_idx = train[target==1].sig_id.unique()\n    select_idx = [i for i in sig_id_lst if i not in del_idx]\n    print(f\"neg labels: {len(sig_id_lst)}→ selected neg labels: {len(select_idx)}\")\n    \n    # Select negative target\n    _df = _df.set_index('sig_id')\n    _df = _df.loc[select_idx, :]\n    _df = _df.reset_index(drop=True)\n    \n    _df[\"target\"] = 0\n    \n    return _df","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#===========================================================\n# model\n#===========================================================\ndef run_lgbm(target_col: str):\n    target = get_target(target_col, annot_sig)\n    target_rate = target.sum() / len(target)\n    \n    # Estimate test target rate\n    #target_rate *= (-0.001*target.sum()+1.1)\n    Adj_target_rate =  (2 * target_rate)  / (target.sum()**0.15)\n    \n    trt = train[target==1].copy().reset_index(drop=True)\n    trt[\"target\"] = 1\n    trt = trt.drop(\"sig_id\", axis=1)\n    \n    logger.info(f\"{target_col}, len(trt):{len(trt)}, target_rate:{target_rate:.7f} → Adj_target_rate:{Adj_target_rate:.7f}\")\n    \n    othr =  Multi_Stratification(train, target_col, target)\n        \n    X_train = pd.concat([trt, othr], axis=0, sort=False, ignore_index=True)   \n    y_train = X_train[\"target\"]\n    X_train = X_train.drop(\"target\", axis=1)\n\n    sm = SMOTE(0.1, k_neighbors=3, n_jobs=2, random_state=SEED)\n    X_train, y_train = sm.fit_sample(X_train, y_train)\n    \n    X_test = test.drop(\"sig_id\", axis=1)\n    \n    train_X, train_y, feature_importance_df_ = pseudo_labeling(X_train, y_train, X_test, target_rate, target_col)\n    \n    y_preds = []\n    models = []\n    oof_train = np.zeros((len(train_X),))\n    score = 0\n        \n    for fold_, (train_index, valid_index) in enumerate(cv.split(train_X, train_y)):\n        logger.info(f'len(train_index) : {len(train_index)}')\n        logger.info(f'len(valid_index) : {len(valid_index)}')\n        \n        X_tr = train_X.loc[train_index, :]\n        X_val = train_X.loc[valid_index, :]\n        y_tr = train_y[train_index]\n        y_val = train_y[valid_index]\n\n        lgb_train = lgb.Dataset(X_tr,\n                                y_tr,\n                                categorical_feature=categorical_cols)\n\n        lgb_eval = lgb.Dataset(X_val,\n                               y_val,\n                               reference=lgb_train,\n                               categorical_feature=categorical_cols)\n        \n        logger.info(f\"================================= fold {fold_+1}/{cv.get_n_splits()} {target_col}=================================\")\n        \n        \n        model = lgb.train(params,\n                          lgb_train,\n                          valid_sets=[lgb_train, lgb_eval],\n                          verbose_eval=100,\n                          num_boost_round=Num_boost_round,\n                          early_stopping_rounds=Early_stopping_rounds)\n        \n        oof_train[valid_index] = model.predict(X_val, num_iteration=model.best_iteration)\n\n        y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n        y_preds.append(y_pred)\n        models.append(model)\n    \n    score = log_loss(train_y, oof_train)\n    \n    logger.info(f\"{target_col} logloss: {score}\")\n    logger.info(f\"=========================================================================================\")\n    \n    return sum(y_preds) / len(y_preds), score, models, feature_importance_df_","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_label(df, conf_0, conf_1, threshold=0.5):\n    df = df.copy()\n    Probability = df.iloc[:,0]\n    # Remove low confidence labels\n    conf_index = df[(Probability <= conf_0) & (conf_1 <= Probability)].index.values\n    \n    Probability = Probability.where(Probability < threshold, 1).copy()\n    p_label = Probability.where(Probability >= threshold, 0).copy()\n    \n    return p_label, conf_index","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_params = {\n    'max_depth': Max_depth,\n    'num_leaves': int((Max_depth**2)*0.7),\n    'n_estimators': Num_boost_round,\n    'learning_rate': 0.03,\n    'objective': \"binary\",\n    'colsample_bytree':0.4,\n    'subsample':0.8,\n    'subsample_freq':5,\n    'reg_alpha':0.1,\n    'reg_lambda':0.1,\n    'random_state':SEED,\n    'n_jobs':2,\n}","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#===========================================================\n# pseudo_labeling\n#===========================================================\n\ndef pseudo_labeling(X_train, y_train, X_test, target_rate, target_col, max_iter=3):\n       \n    X = X_train.copy()\n    y = y_train.copy()\n    feature_importance_df = pd.DataFrame()\n    \n    for iter_ in range(1, max_iter+1):\n    \n        logger.info(f\"================= Pseudo labeling {iter_} / {max_iter} =================\")\n        \n        y_preds = np.zeros((X.shape[0], 2))\n        y_preds[:, 0] = y.copy()\n        \n        y_prob = np.zeros((X_test.shape[0]))\n        \n        X_conf = pd.DataFrame()\n        y_conf = pd.DataFrame()\n        _importance_df = pd.DataFrame()\n        _importance_df[\"Feature\"] = X.columns\n        \n        for fold_, (train_idx, valid_idx) in enumerate(cv.split(X, y)):        \n            X_tr, X_val = X.loc[train_idx, :], X.loc[valid_idx, :]\n            y_tr, y_val = y[train_idx], y[valid_idx]\n            \n            clf = LGBMClassifier(**classifier_params)\n            \n            clf.fit(X_tr, y_tr,\n                    eval_set=[(X_tr, y_tr), (X_val, y_val)],\n                    eval_metric='logloss',\n                    verbose=100,\n                    early_stopping_rounds=Early_stopping_rounds)\n            \n            y_preds[valid_idx, 1] = clf.predict_proba(X_val, num_iteration=clf.best_iteration_)[:, 1]\n            y_prob += clf.predict_proba(X_test, num_iteration=clf.best_iteration_)[:, 1] / N_FOLD\n            \n            # feature importance with target col\n            _importance_df[\"importance\"] = clf.feature_importances_\n            feature_importance_df = pd.concat([feature_importance_df, _importance_df], axis=0)\n        \n        auc_score = roc_auc_score(y_preds[:, 0], y_preds[:, 1])\n        logger.info(f\"{iter_} / {max_iter} AUC score:{auc_score:.3f}\") \n        y_preds = pd.DataFrame(y_preds, index=X.index, columns=[[\"Labels\", \"Preds\"]])\n        \n        if iter_ == 1: Threshold = y_preds.iloc[:, 1].quantile(0.89)\n            \n        logger.info(f\"Threshold: {Threshold}\")\n            \n        y_preds.iloc[:,1] = y_preds.iloc[:,1].where(y_preds.iloc[:,1] < Threshold, 1).copy()\n        y_preds.iloc[:,1] = y_preds.iloc[:,1].where(y_preds.iloc[:,1] >= Threshold, 0).copy()\n        y_preds = y_preds.sum(axis=1)\n                \n        corect_idx = y_preds[y_preds != 1].index.values\n        X_corect, y_corect = X[X.index.isin(corect_idx)], y[y.index.isin(corect_idx)]\n        \n        logger.info(f\"Remove_noisy_labels: {len(y)-len(y_corect)} → positive_corect_labels: {sum(y_corect)}/{len(y_corect)}\")\n            \n        # Remove low confidence labels\n        y_prob = pd.DataFrame(y_prob, index=X_test.index, columns=[\"probability\"])\n        \n        percentile = y_prob.probability.quantile(0.3)\n        high_conf_0 = min(y_prob.probability.min()*30, percentile)\n        high_conf_1 = max(y_prob.probability.max()*0.6,Threshold)\n        logger.info(f\"30th percentile: {percentile:.7f}\")\n        \n        p_label, conf_idx = convert_label(y_prob, high_conf_0, high_conf_1, Threshold)\n        \n        p_label_rate = sum(p_label)/len(p_label)        \n        logger.info(f\"p_label_rate: {p_label_rate:.7f} Vs.target_rate: {target_rate:.5f}, Num_p_label: {sum(p_label)}, conf_0:{high_conf_0:.5f}, conf_1:{high_conf_1:.5f}\")\n        \n        # Set the params of threshold based on train labels rate (target_rate).\n        # target_rate = target.sum() / len(target)\n        \n        if p_label_rate*3 < target_rate:\n            check = len(y_prob)*target_rate\n            for i in range(10):\n                logger.info(f\"Num_p_label: {sum(p_label)}, Expected: {check:.1f}, Adj_threshold_{i+1}: {Threshold:.7f}\")\n                if sum(p_label)*1.5 >= check: break \n                if (Threshold-0.005) < 0: break\n                Threshold -= 0.005\n                high_conf_1 = max(y_prob.probability.max()*0.6,Threshold)\n                p_label, conf_idx = convert_label(y_prob, high_conf_0, high_conf_1, Threshold)\n               \n                \n        if p_label_rate > target_rate*3:\n            check = len(y_prob)*target_rate\n            for i in range(10):\n                logger.info(f\"Num_p_label: {sum(p_label)}, Expected: {check:.1f}, Adj_threshold_{i+1}: {Threshold:.7f}\")\n                if sum(p_label) <= check*1.5: break\n                if (Threshold+0.005) > 0.99: break\n                Threshold += 0.005\n                high_conf_1 = max(y_prob.probability.max()*0.6,Threshold)\n                p_label, conf_idx = convert_label(y_prob, high_conf_0, high_conf_1, Threshold)\n                \n        if iter_ == max_iter:\n            X_conf = X_test.copy()\n        else:\n            X_conf = X_test[X_test.index.isin(conf_idx)].copy()\n            \n        logger.info(f\"threshold:{Threshold:.7f}, positive p_label:{sum(p_label)}/{len(p_label)}, p_label_rate: {sum(p_label)/len(p_label):.7f}\")\n \n        X = pd.concat([X_corect, X_conf], axis=0, ignore_index=True)\n        y = pd.concat([y_corect, p_label], axis=0, ignore_index=True)\n        \n        X = X.drop_duplicates(keep=\"last\").reset_index(drop=True)\n        y = y[X.index.values].reset_index(drop=True)\n        \n        logger.info(f\"positive y_label:{sum(y)}/{len(y)}, y_label_rate: {sum(y)/len(y):.7f}\")\n        \n    if DEBUG:\n        show_feature_importance(feature_importance_df, target_col, num=10)\n        \n    return X, y, feature_importance_df","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = []\nfeature_importance_df = pd.DataFrame()\nimportance_cols_df = pd.DataFrame()\nscores = []\nmodels = []\n\nfor target_col in tqdm(train_targets_scored.columns[1:]):\n    _preds, _score, models, _feature_importance_df = run_lgbm(target_col)\n\n    sub[target_col] = _preds\n    scores.append(_score)\n    \n    if DEBUG:\n        if _score > 0.02:\n            importance_cols_df[target_col] = select_importance_cols(_feature_importance_df)\n            print(importance_cols_df)\n        \n        feature_importance_df = create_featureimprotance(models, feature_importance_df)","execution_count":68,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=206.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bc22daf90904bd5b170364cefdc92fe"}},"metadata":{}},{"output_type":"stream","text":"5-alpha_reductase_inhibitor, len(trt):17, target_rate:0.0007139→Adj_target_rate:0.0009334\n","name":"stderr"},{"output_type":"stream","text":"neg labels: 3289→ selected neg labels: 3286\n","name":"stdout"},{"output_type":"stream","text":"================= Pseudo labeling 1 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0135452\tvalid_1's binary_logloss: 0.0191472\n[200]\ttraining's binary_logloss: 0.00183751\tvalid_1's binary_logloss: 0.00483274\n[300]\ttraining's binary_logloss: 0.000619462\tvalid_1's binary_logloss: 0.00244618\n[400]\ttraining's binary_logloss: 0.000364677\tvalid_1's binary_logloss: 0.00178096\nEarly stopping, best iteration is:\n[447]\ttraining's binary_logloss: 0.000329967\tvalid_1's binary_logloss: 0.0016924\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0134128\tvalid_1's binary_logloss: 0.0251687\n[200]\ttraining's binary_logloss: 0.00177434\tvalid_1's binary_logloss: 0.0103178\n[300]\ttraining's binary_logloss: 0.000588688\tvalid_1's binary_logloss: 0.00808499\n","name":"stdout"},{"output_type":"stream","text":"1 / 3 AUC score:1.000\nThreshold: 0.0028936071142916997\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[316]\ttraining's binary_logloss: 0.000524937\tvalid_1's binary_logloss: 0.00792287\n","name":"stdout"},{"output_type":"stream","text":"Remove_noisy_labels: 70 → positive_corect_labels: 328/3544\n30th percentile: 0.0001332\np_label_rate: 0.0203415 Vs.target_rate: 0.00071, Num_p_label: 81.0, conf_0:0.00013, conf_1:0.25384\nNum_p_label: 81.0, Expected: 2.8, Adj_threshold_1: 0.0028936\nNum_p_label: 4.0, Expected: 2.8, Adj_threshold_2: 0.0428936\nthreshold:0.0428936, positive p_label:4.0/3982, p_label_rate: 0.0010045\npositive y_label:328.0/3544, y_label_rate: 0.0925508\n================= Pseudo labeling 2 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0137119\tvalid_1's binary_logloss: 0.0207253\n[200]\ttraining's binary_logloss: 0.00182282\tvalid_1's binary_logloss: 0.005684\n[300]\ttraining's binary_logloss: 0.000613737\tvalid_1's binary_logloss: 0.00319211\n[400]\ttraining's binary_logloss: 0.000359303\tvalid_1's binary_logloss: 0.00251998\nEarly stopping, best iteration is:\n[419]\ttraining's binary_logloss: 0.000340245\tvalid_1's binary_logloss: 0.00246295\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.013438\tvalid_1's binary_logloss: 0.0251803\n[200]\ttraining's binary_logloss: 0.00173803\tvalid_1's binary_logloss: 0.00958552\n","name":"stdout"},{"output_type":"stream","text":"2 / 3 AUC score:1.000\nThreshold: 0.0428936071142917\nRemove_noisy_labels: 2 → positive_corect_labels: 326.0/3542\n30th percentile: 0.0001691\np_label_rate: 0.0015068 Vs.target_rate: 0.00071, Num_p_label: 6.0, conf_0:0.00017, conf_1:0.37133\nthreshold:0.0428936, positive p_label:6.0/3982, p_label_rate: 0.0015068\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[285]\ttraining's binary_logloss: 0.000647369\tvalid_1's binary_logloss: 0.00775384\n","name":"stdout"},{"output_type":"stream","text":"positive y_label:326.0/3542, y_label_rate: 0.0920384\n================= Pseudo labeling 3 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0132411\tvalid_1's binary_logloss: 0.022991\n[200]\ttraining's binary_logloss: 0.00174334\tvalid_1's binary_logloss: 0.00780631\n[300]\ttraining's binary_logloss: 0.000586369\tvalid_1's binary_logloss: 0.00463795\n[400]\ttraining's binary_logloss: 0.000348146\tvalid_1's binary_logloss: 0.00363468\nEarly stopping, best iteration is:\n[470]\ttraining's binary_logloss: 0.00031287\tvalid_1's binary_logloss: 0.00348009\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0133666\tvalid_1's binary_logloss: 0.0188077\n[200]\ttraining's binary_logloss: 0.00171827\tvalid_1's binary_logloss: 0.00398344\n[300]\ttraining's binary_logloss: 0.000581714\tvalid_1's binary_logloss: 0.00182374\n[400]\ttraining's binary_logloss: 0.000351756\tvalid_1's binary_logloss: 0.00131856\n","name":"stdout"},{"output_type":"stream","text":"3 / 3 AUC score:1.000\nThreshold: 0.0428936071142917\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[465]\ttraining's binary_logloss: 0.000311073\tvalid_1's binary_logloss: 0.00121136\n","name":"stdout"},{"output_type":"stream","text":"Remove_noisy_labels: 0 → positive_corect_labels: 326.0/3542\n30th percentile: 0.0000896\np_label_rate: 0.0010045 Vs.target_rate: 0.00071, Num_p_label: 4.0, conf_0:0.00009, conf_1:0.39892\nthreshold:0.0428936, positive p_label:4.0/3982, p_label_rate: 0.0010045\npositive y_label:330.0/7524, y_label_rate: 0.0438596\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x216 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","text":"len(train_index) : 3762\nlen(valid_index) : 3762\n================================= fold 1/2 5-alpha_reductase_inhibitor=================================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.00649303\tvalid_1's binary_logloss: 0.0195198\n[200]\ttraining's binary_logloss: 0.000846135\tvalid_1's binary_logloss: 0.012958\n","name":"stdout"},{"output_type":"stream","text":"len(train_index) : 3762\nlen(valid_index) : 3762\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[259]\ttraining's binary_logloss: 0.000391805\tvalid_1's binary_logloss: 0.011717\n","name":"stdout"},{"output_type":"stream","text":"================================= fold 2/2 5-alpha_reductase_inhibitor=================================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.00640919\tvalid_1's binary_logloss: 0.0127781\n[200]\ttraining's binary_logloss: 0.000827386\tvalid_1's binary_logloss: 0.00480246\n[300]\ttraining's binary_logloss: 0.000280506\tvalid_1's binary_logloss: 0.00360585\nEarly stopping, best iteration is:\n[388]\ttraining's binary_logloss: 0.000176234\tvalid_1's binary_logloss: 0.00330738\n","name":"stdout"},{"output_type":"stream","text":"5-alpha_reductase_inhibitor logloss: 0.007512191012065222\n=========================================================================================\n11-beta-hsd1_inhibitor, len(trt):18, target_rate:0.0007559→Adj_target_rate:0.0009799\n","name":"stderr"},{"output_type":"stream","text":"neg labels: 3289→ selected neg labels: 3286\n","name":"stdout"},{"output_type":"stream","text":"================= Pseudo labeling 1 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0165354\tvalid_1's binary_logloss: 0.0316597\n[200]\ttraining's binary_logloss: 0.00233086\tvalid_1's binary_logloss: 0.0110646\n[300]\ttraining's binary_logloss: 0.000763447\tvalid_1's binary_logloss: 0.00650142\n[400]\ttraining's binary_logloss: 0.000439987\tvalid_1's binary_logloss: 0.00530776\nEarly stopping, best iteration is:\n[474]\ttraining's binary_logloss: 0.000368928\tvalid_1's binary_logloss: 0.00493455\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0166916\tvalid_1's binary_logloss: 0.0281535\n[200]\ttraining's binary_logloss: 0.00233176\tvalid_1's binary_logloss: 0.00763114\n[300]\ttraining's binary_logloss: 0.000760919\tvalid_1's binary_logloss: 0.00419317\n[400]\ttraining's binary_logloss: 0.000440472\tvalid_1's binary_logloss: 0.00332854\nEarly stopping, best iteration is:\n[452]\ttraining's binary_logloss: 0.000382552\tvalid_1's binary_logloss: 0.003158\n","name":"stdout"},{"output_type":"stream","text":"1 / 3 AUC score:1.000\nThreshold: 0.0030000928475512763\nRemove_noisy_labels: 70 → positive_corect_labels: 328/3544\n30th percentile: 0.0001207\np_label_rate: 0.0188348 Vs.target_rate: 0.00076, Num_p_label: 75.0, conf_0:0.00012, conf_1:0.01791\nNum_p_label: 75.0, Expected: 3.0, Adj_threshold_1: 0.0030001\nNum_p_label: 0.0, Expected: 3.0, Adj_threshold_2: 0.0430001\nthreshold:0.0430001, positive p_label:0.0/3982, p_label_rate: 0.0000000\npositive y_label:328.0/3544, y_label_rate: 0.0925508\n================= Pseudo labeling 2 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0160231\tvalid_1's binary_logloss: 0.032151\n[200]\ttraining's binary_logloss: 0.00223659\tvalid_1's binary_logloss: 0.0118237\n[300]\ttraining's binary_logloss: 0.000740099\tvalid_1's binary_logloss: 0.00752002\n[400]\ttraining's binary_logloss: 0.000426615\tvalid_1's binary_logloss: 0.00598342\nEarly stopping, best iteration is:\n[421]\ttraining's binary_logloss: 0.000398976\tvalid_1's binary_logloss: 0.00585158\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0167961\tvalid_1's binary_logloss: 0.0267049\n[200]\ttraining's binary_logloss: 0.00234679\tvalid_1's binary_logloss: 0.00773743\n[300]\ttraining's binary_logloss: 0.000773839\tvalid_1's binary_logloss: 0.00402483\n[400]\ttraining's binary_logloss: 0.000445908\tvalid_1's binary_logloss: 0.00328437\nEarly stopping, best iteration is:\n[422]\ttraining's binary_logloss: 0.000413327\tvalid_1's binary_logloss: 0.00315001\n","name":"stdout"},{"output_type":"stream","text":"2 / 3 AUC score:1.000\nThreshold: 0.043000092847551276\nRemove_noisy_labels: 1 → positive_corect_labels: 328.0/3543\n30th percentile: 0.0001382\np_label_rate: 0.0010045 Vs.target_rate: 0.00076, Num_p_label: 4.0, conf_0:0.00014, conf_1:0.04300\nthreshold:0.0430001, positive p_label:4.0/3982, p_label_rate: 0.0010045\npositive y_label:328.0/3543, y_label_rate: 0.0925769\n================= Pseudo labeling 3 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0159847\tvalid_1's binary_logloss: 0.0331829\n[200]\ttraining's binary_logloss: 0.00224377\tvalid_1's binary_logloss: 0.0135625\n[300]\ttraining's binary_logloss: 0.000740126\tvalid_1's binary_logloss: 0.0089535\n[400]\ttraining's binary_logloss: 0.00043005\tvalid_1's binary_logloss: 0.00752387\nEarly stopping, best iteration is:\n[434]\ttraining's binary_logloss: 0.000391958\tvalid_1's binary_logloss: 0.00725329\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0165518\tvalid_1's binary_logloss: 0.027808\n[200]\ttraining's binary_logloss: 0.00231199\tvalid_1's binary_logloss: 0.00794811\n[300]\ttraining's binary_logloss: 0.000764085\tvalid_1's binary_logloss: 0.00424105\n[400]\ttraining's binary_logloss: 0.000441958\tvalid_1's binary_logloss: 0.00343202\nEarly stopping, best iteration is:\n[420]\ttraining's binary_logloss: 0.00041311\tvalid_1's binary_logloss: 0.00330965\n","name":"stdout"},{"output_type":"stream","text":"3 / 3 AUC score:1.000\nThreshold: 0.043000092847551276\nRemove_noisy_labels: 0 → positive_corect_labels: 328.0/3543\n30th percentile: 0.0001355\np_label_rate: 0.0005023 Vs.target_rate: 0.00076, Num_p_label: 2.0, conf_0:0.00014, conf_1:0.04300\nthreshold:0.0430001, positive p_label:2.0/3982, p_label_rate: 0.0005023\npositive y_label:330.0/7525, y_label_rate: 0.0438538\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x216 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","text":"len(train_index) : 3762\nlen(valid_index) : 3763\n================================= fold 1/2 11-beta-hsd1_inhibitor=================================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.00837517\tvalid_1's binary_logloss: 0.0170526\n[200]\ttraining's binary_logloss: 0.00115896\tvalid_1's binary_logloss: 0.00816378\n","name":"stdout"},{"output_type":"stream","text":"len(train_index) : 3763\nlen(valid_index) : 3762\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[256]\ttraining's binary_logloss: 0.000551612\tvalid_1's binary_logloss: 0.00734211\n","name":"stdout"},{"output_type":"stream","text":"================================= fold 2/2 11-beta-hsd1_inhibitor=================================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.00835052\tvalid_1's binary_logloss: 0.0150263\n[200]\ttraining's binary_logloss: 0.00119011\tvalid_1's binary_logloss: 0.00542546\n[300]\ttraining's binary_logloss: 0.000375975\tvalid_1's binary_logloss: 0.00366339\nEarly stopping, best iteration is:\n[365]\ttraining's binary_logloss: 0.000246058\tvalid_1's binary_logloss: 0.00339916\n","name":"stdout"},{"output_type":"stream","text":"11-beta-hsd1_inhibitor logloss: 0.005370900574502703\n=========================================================================================\nacat_inhibitor, len(trt):24, target_rate:0.0010078→Adj_target_rate:0.0012513\n","name":"stderr"},{"output_type":"stream","text":"neg labels: 3289→ selected neg labels: 3285\n","name":"stdout"},{"output_type":"stream","text":"================= Pseudo labeling 1 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0219726\tvalid_1's binary_logloss: 0.0403129\n[200]\ttraining's binary_logloss: 0.00332899\tvalid_1's binary_logloss: 0.0128994\n[300]\ttraining's binary_logloss: 0.00106364\tvalid_1's binary_logloss: 0.00707104\n[400]\ttraining's binary_logloss: 0.000576954\tvalid_1's binary_logloss: 0.00544064\nEarly stopping, best iteration is:\n[420]\ttraining's binary_logloss: 0.000529638\tvalid_1's binary_logloss: 0.00527676\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0213812\tvalid_1's binary_logloss: 0.0442957\n[200]\ttraining's binary_logloss: 0.00344318\tvalid_1's binary_logloss: 0.0146807\n[300]\ttraining's binary_logloss: 0.00108729\tvalid_1's binary_logloss: 0.00814334\n[400]\ttraining's binary_logloss: 0.000575235\tvalid_1's binary_logloss: 0.00607786\nEarly stopping, best iteration is:\n[427]\ttraining's binary_logloss: 0.000514504\tvalid_1's binary_logloss: 0.005989\n","name":"stdout"},{"output_type":"stream","text":"1 / 3 AUC score:1.000\nThreshold: 0.002842034744329916\nRemove_noisy_labels: 70 → positive_corect_labels: 328/3543\n30th percentile: 0.0002338\np_label_rate: 0.0215972 Vs.target_rate: 0.00101, Num_p_label: 86.0, conf_0:0.00023, conf_1:0.01275\nNum_p_label: 86.0, Expected: 4.0, Adj_threshold_1: 0.0028420\nNum_p_label: 0.0, Expected: 4.0, Adj_threshold_2: 0.0428420\nthreshold:0.0428420, positive p_label:0.0/3982, p_label_rate: 0.0000000\npositive y_label:328.0/3543, y_label_rate: 0.0925769\n================= Pseudo labeling 2 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.021786\tvalid_1's binary_logloss: 0.0377256\n[200]\ttraining's binary_logloss: 0.00341752\tvalid_1's binary_logloss: 0.0113919\n[300]\ttraining's binary_logloss: 0.00107965\tvalid_1's binary_logloss: 0.00613588\n[400]\ttraining's binary_logloss: 0.000577734\tvalid_1's binary_logloss: 0.00474877\nEarly stopping, best iteration is:\n[461]\ttraining's binary_logloss: 0.000472139\tvalid_1's binary_logloss: 0.00441528\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0204355\tvalid_1's binary_logloss: 0.0429225\n[200]\ttraining's binary_logloss: 0.00325726\tvalid_1's binary_logloss: 0.0137834\n[300]\ttraining's binary_logloss: 0.00105351\tvalid_1's binary_logloss: 0.00734434\n","name":"stdout"},{"output_type":"stream","text":"2 / 3 AUC score:1.000\nThreshold: 0.04284203474432992\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[344]\ttraining's binary_logloss: 0.000756378\tvalid_1's binary_logloss: 0.0063642\n","name":"stdout"},{"output_type":"stream","text":"Remove_noisy_labels: 0 → positive_corect_labels: 328.0/3543\n30th percentile: 0.0002838\np_label_rate: 0.0002511 Vs.target_rate: 0.00101, Num_p_label: 1.0, conf_0:0.00028, conf_1:0.04284\nNum_p_label: 1.0, Expected: 4.0, Adj_threshold_1: 0.0428420\nthreshold:0.0028420, positive p_label:1.0/3982, p_label_rate: 0.0002511\npositive y_label:328.0/3543, y_label_rate: 0.0925769\n================= Pseudo labeling 3 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.021786\tvalid_1's binary_logloss: 0.0377256\n[200]\ttraining's binary_logloss: 0.00341752\tvalid_1's binary_logloss: 0.0113919\n[300]\ttraining's binary_logloss: 0.00107965\tvalid_1's binary_logloss: 0.00613588\n[400]\ttraining's binary_logloss: 0.000577734\tvalid_1's binary_logloss: 0.00474877\nEarly stopping, best iteration is:\n[461]\ttraining's binary_logloss: 0.000472139\tvalid_1's binary_logloss: 0.00441528\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0204355\tvalid_1's binary_logloss: 0.0429225\n[200]\ttraining's binary_logloss: 0.00325726\tvalid_1's binary_logloss: 0.0137834\n[300]\ttraining's binary_logloss: 0.00105351\tvalid_1's binary_logloss: 0.00734434\n","name":"stdout"},{"output_type":"stream","text":"3 / 3 AUC score:1.000\nThreshold: 0.0028420347443299174\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[344]\ttraining's binary_logloss: 0.000756378\tvalid_1's binary_logloss: 0.0063642\n","name":"stdout"},{"output_type":"stream","text":"Remove_noisy_labels: 89 → positive_corect_labels: 328.0/3454\n30th percentile: 0.0002838\np_label_rate: 0.0356605 Vs.target_rate: 0.00101, Num_p_label: 142.0, conf_0:0.00028, conf_1:0.03186\nNum_p_label: 142.0, Expected: 4.0, Adj_threshold_1: 0.0028420\nNum_p_label: 1.0, Expected: 4.0, Adj_threshold_2: 0.0428420\nthreshold:0.0428420, positive p_label:1.0/3982, p_label_rate: 0.0002511\npositive y_label:329.0/7436, y_label_rate: 0.0442442\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x216 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","text":"len(train_index) : 3718\nlen(valid_index) : 3718\n================================= fold 1/2 acat_inhibitor=================================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0104446\tvalid_1's binary_logloss: 0.0200541\n[200]\ttraining's binary_logloss: 0.00157964\tvalid_1's binary_logloss: 0.00704974\n[300]\ttraining's binary_logloss: 0.000490561\tvalid_1's binary_logloss: 0.00452111\n[400]\ttraining's binary_logloss: 0.000265756\tvalid_1's binary_logloss: 0.00386762\nEarly stopping, best iteration is:\n[426]\ttraining's binary_logloss: 0.000240211\tvalid_1's binary_logloss: 0.00374949\n","name":"stdout"},{"output_type":"stream","text":"len(train_index) : 3718\nlen(valid_index) : 3718\n================================= fold 2/2 acat_inhibitor=================================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0107119\tvalid_1's binary_logloss: 0.0190614\n[200]\ttraining's binary_logloss: 0.00170319\tvalid_1's binary_logloss: 0.00680461\n[300]\ttraining's binary_logloss: 0.000530666\tvalid_1's binary_logloss: 0.00425386\nEarly stopping, best iteration is:\n[385]\ttraining's binary_logloss: 0.000307914\tvalid_1's binary_logloss: 0.00331397\n","name":"stdout"},{"output_type":"stream","text":"acat_inhibitor logloss: 0.003531729592869254\n=========================================================================================\nacetylcholine_receptor_agonist, len(trt):190, target_rate:0.0079785→Adj_target_rate:0.0072634\n","name":"stderr"},{"output_type":"stream","text":"neg labels: 3289→ selected neg labels: 3259\n","name":"stdout"},{"output_type":"stream","text":"================= Pseudo labeling 1 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0649045\tvalid_1's binary_logloss: 0.234036\nEarly stopping, best iteration is:\n[124]\ttraining's binary_logloss: 0.0485675\tvalid_1's binary_logloss: 0.228998\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0623978\tvalid_1's binary_logloss: 0.244025\n","name":"stdout"},{"output_type":"stream","text":"1 / 3 AUC score:0.815\nThreshold: 0.07858107329055186\nRemove_noisy_labels: 368 → positive_corect_labels: 176/3216\n30th percentile: 0.0230493\np_label_rate: 0.0539930 Vs.target_rate: 0.00798, Num_p_label: 215.0, conf_0:0.02305, conf_1:0.09758\nNum_p_label: 215.0, Expected: 31.8, Adj_threshold_1: 0.0785811\nNum_p_label: 28.0, Expected: 31.8, Adj_threshold_2: 0.1185811\nthreshold:0.1185811, positive p_label:28.0/3982, p_label_rate: 0.0070316\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[125]\ttraining's binary_logloss: 0.0452594\tvalid_1's binary_logloss: 0.242474\n","name":"stdout"},{"output_type":"stream","text":"positive y_label:176.0/3216, y_label_rate: 0.0547264\n================= Pseudo labeling 2 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0258081\tvalid_1's binary_logloss: 0.14341\nEarly stopping, best iteration is:\n[109]\ttraining's binary_logloss: 0.0224373\tvalid_1's binary_logloss: 0.142834\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.026407\tvalid_1's binary_logloss: 0.147177\n","name":"stdout"},{"output_type":"stream","text":"2 / 3 AUC score:0.902\nThreshold: 0.11858107329055187\nRemove_noisy_labels: 100 → positive_corect_labels: 76.0/3116\n30th percentile: 0.0099862\np_label_rate: 0.0002511 Vs.target_rate: 0.00798, Num_p_label: 1.0, conf_0:0.00999, conf_1:0.11858\nNum_p_label: 1.0, Expected: 31.8, Adj_threshold_1: 0.1185811\nthreshold:0.0785811, positive p_label:1.0/3982, p_label_rate: 0.0002511\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[102]\ttraining's binary_logloss: 0.0255504\tvalid_1's binary_logloss: 0.147079\n","name":"stdout"},{"output_type":"stream","text":"positive y_label:76.0/3116, y_label_rate: 0.0243902\n================= Pseudo labeling 3 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[77]\ttraining's binary_logloss: 0.0146876\tvalid_1's binary_logloss: 0.0704778\nTraining until validation scores don't improve for 10 rounds\n","name":"stdout"},{"output_type":"stream","text":"3 / 3 AUC score:0.877\nThreshold: 0.07858107329055186\nRemove_noisy_labels: 42 → positive_corect_labels: 34.0/3074\n30th percentile: 0.0048005\np_label_rate: 0.0000000 Vs.target_rate: 0.00798, Num_p_label: 0.0, conf_0:0.00480, conf_1:0.07858\nNum_p_label: 0.0, Expected: 31.8, Adj_threshold_1: 0.0785811\nthreshold:0.0385811, positive p_label:0.0/3982, p_label_rate: 0.0000000\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[79]\ttraining's binary_logloss: 0.0120071\tvalid_1's binary_logloss: 0.0841502\n","name":"stdout"},{"output_type":"stream","text":"positive y_label:34.0/7056, y_label_rate: 0.0048186\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x216 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","text":"len(train_index) : 3528\nlen(valid_index) : 3528\n================================= fold 1/2 acetylcholine_receptor_agonist=================================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.00151433\tvalid_1's binary_logloss: 0.0136231\n","name":"stdout"},{"output_type":"stream","text":"len(train_index) : 3528\nlen(valid_index) : 3528\n================================= fold 2/2 acetylcholine_receptor_agonist=================================\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[120]\ttraining's binary_logloss: 0.00100265\tvalid_1's binary_logloss: 0.0129077\nTraining until validation scores don't improve for 10 rounds\n","name":"stdout"},{"output_type":"stream","text":"acetylcholine_receptor_agonist logloss: 0.01328063959734726\n=========================================================================================\n","name":"stderr"},{"output_type":"stream","text":"[100]\ttraining's binary_logloss: 0.00132936\tvalid_1's binary_logloss: 0.0137801\nEarly stopping, best iteration is:\n[95]\ttraining's binary_logloss: 0.00144165\tvalid_1's binary_logloss: 0.0136535\n","name":"stdout"},{"output_type":"stream","text":"acetylcholine_receptor_antagonist, len(trt):301, target_rate:0.0126396→Adj_target_rate:0.0107394\n","name":"stderr"},{"output_type":"stream","text":"neg labels: 3289→ selected neg labels: 3241\n","name":"stdout"},{"output_type":"stream","text":"================= Pseudo labeling 1 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[38]\ttraining's binary_logloss: 0.161667\tvalid_1's binary_logloss: 0.299128\nTraining until validation scores don't improve for 10 rounds\n","name":"stdout"},{"output_type":"stream","text":"1 / 3 AUC score:0.569\nThreshold: 0.11014992936981466\nRemove_noisy_labels: 617 → positive_corect_labels: 50/2948\n30th percentile: 0.0718249\np_label_rate: 0.0630337 Vs.target_rate: 0.01264, Num_p_label: 251.0, conf_0:0.07182, conf_1:0.11015\nNum_p_label: 251.0, Expected: 50.3, Adj_threshold_1: 0.1101499\nNum_p_label: 3.0, Expected: 50.3, Adj_threshold_2: 0.1501499\nthreshold:0.1501499, positive p_label:3.0/3982, p_label_rate: 0.0007534\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[24]\ttraining's binary_logloss: 0.199686\tvalid_1's binary_logloss: 0.299656\n","name":"stdout"},{"output_type":"stream","text":"positive y_label:50.0/2948, y_label_rate: 0.0169607\n================= Pseudo labeling 2 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[35]\ttraining's binary_logloss: 0.0266438\tvalid_1's binary_logloss: 0.0800122\nTraining until validation scores don't improve for 10 rounds\n","name":"stdout"},{"output_type":"stream","text":"2 / 3 AUC score:0.576\nThreshold: 0.15014992936981467\nRemove_noisy_labels: 44 → positive_corect_labels: 6.0/2904\n30th percentile: 0.0084819\np_label_rate: 0.0000000 Vs.target_rate: 0.01264, Num_p_label: 0.0, conf_0:0.00848, conf_1:0.15015\nNum_p_label: 0.0, Expected: 50.3, Adj_threshold_1: 0.1501499\nthreshold:0.1101499, positive p_label:0.0/3982, p_label_rate: 0.0000000\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[37]\ttraining's binary_logloss: 0.0254114\tvalid_1's binary_logloss: 0.0805699\n","name":"stdout"},{"output_type":"stream","text":"positive y_label:6.0/2904, y_label_rate: 0.0020661\n================= Pseudo labeling 3 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[25]\ttraining's binary_logloss: 0.00471798\tvalid_1's binary_logloss: 0.0109034\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[25]\ttraining's binary_logloss: 0.00437687\tvalid_1's binary_logloss: 0.012341\n","name":"stdout"},{"output_type":"stream","text":"3 / 3 AUC score:0.924\nThreshold: 0.11014992936981466\nRemove_noisy_labels: 6 → positive_corect_labels: 0.0/2898\n30th percentile: 0.0010547\np_label_rate: 0.0000000 Vs.target_rate: 0.01264, Num_p_label: 0.0, conf_0:0.00105, conf_1:0.11015\nNum_p_label: 0.0, Expected: 50.3, Adj_threshold_1: 0.1101499\nthreshold:0.0701499, positive p_label:0.0/3982, p_label_rate: 0.0000000\npositive y_label:0.0/6880, y_label_rate: 0.0000000\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x216 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","text":"len(train_index) : 3440\nlen(valid_index) : 3440\n================================= fold 1/2 acetylcholine_receptor_antagonist=================================\nlen(train_index) : 3440\nlen(valid_index) : 3440\n================================= fold 2/2 acetylcholine_receptor_antagonist=================================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[1]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\nTraining until validation scores don't improve for 10 rounds\nEarly stopping, best iteration is:\n[1]\ttraining's binary_logloss: 0\tvalid_1's binary_logloss: 0\n\n","name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"y_true contains only one label (0.0). Please provide the true labels explicitly through the labels argument.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-68-66c40746d1da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_targets_scored\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0m_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_feature_importance_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_lgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-62-895d9ece3e69>\u001b[0m in \u001b[0;36mrun_lgbm\u001b[0;34m(target_col)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{target_col} logloss: {score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   2198\u001b[0m             raise ValueError('y_true contains only one label ({0}). Please '\n\u001b[1;32m   2199\u001b[0m                              \u001b[0;34m'provide the true labels explicitly through the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2200\u001b[0;31m                              'labels argument.'.format(lb.classes_[0]))\n\u001b[0m\u001b[1;32m   2201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m             raise ValueError('The labels array needs to contain at least two '\n","\u001b[0;31mValueError\u001b[0m: y_true contains only one label (0.0). Please provide the true labels explicitly through the labels argument."]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_feature_importance(feature_importance_df, title=\"all\", num=100):\n    cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n            .groupby(\"Feature\")\n            .mean()\n            .sort_values(by=\"importance\", ascending=False)[:num].index)\n    \n    best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n    \n    hight = int(num//3.3)\n    plt.figure(figsize=(8, hight))\n    sns.barplot(x=\"importance\", \n                y=\"Feature\", \n                data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title(f'{title}_Features importance (averaged)')\n    plt.tight_layout()\n    plt.savefig(f\"./{title}_feature_importance_{Version}.png\")\n    plt.show()","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\nWe have to convert some categorical features into numbers in train and test. We can identify categorical features by `pd.DataFrame.select_dtypes`."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"         sig_id cp_type  cp_time cp_dose     g-0     g-1     g-2     g-3  \\\n0  id_000644bb2  trt_cp       24      D1  1.0620  0.5577 -0.2479 -0.6208   \n1  id_000779bfc  trt_cp       72      D1  0.0743  0.4087  0.2991  0.0604   \n2  id_000a6266a  trt_cp       48      D1  0.6280  0.5817  1.5540 -0.0764   \n3  id_0015fd391  trt_cp       48      D1 -0.5138 -0.2491 -0.2656  0.5288   \n4  id_001626bd3  trt_cp       72      D2 -0.3254 -0.4009  0.9700  0.6919   \n\n      g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94    c-95  \\\n0 -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912  0.6584   \n1  1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957  0.4899   \n2 -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n3  4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632 -1.2880   \n4  1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523 -0.3031   \n\n     c-96    c-97    c-98    c-99  \n0 -0.3981  0.2139  0.3801  0.4176  \n1  0.1522  0.1241  0.6077  0.7371  \n2 -0.6417 -0.2187 -1.4080  0.6931  \n3 -1.6210 -0.8784 -0.3876 -0.8154  \n4  0.1094  0.2885 -0.3786  0.7125  \n\n[5 rows x 876 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>...</th>\n      <th>c-90</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>trt_cp</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>1.0620</td>\n      <td>0.5577</td>\n      <td>-0.2479</td>\n      <td>-0.6208</td>\n      <td>-0.1944</td>\n      <td>-1.0120</td>\n      <td>...</td>\n      <td>0.2862</td>\n      <td>0.2584</td>\n      <td>0.8076</td>\n      <td>0.5523</td>\n      <td>-0.1912</td>\n      <td>0.6584</td>\n      <td>-0.3981</td>\n      <td>0.2139</td>\n      <td>0.3801</td>\n      <td>0.4176</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_000779bfc</td>\n      <td>trt_cp</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.0743</td>\n      <td>0.4087</td>\n      <td>0.2991</td>\n      <td>0.0604</td>\n      <td>1.0190</td>\n      <td>0.5207</td>\n      <td>...</td>\n      <td>-0.4265</td>\n      <td>0.7543</td>\n      <td>0.4708</td>\n      <td>0.0230</td>\n      <td>0.2957</td>\n      <td>0.4899</td>\n      <td>0.1522</td>\n      <td>0.1241</td>\n      <td>0.6077</td>\n      <td>0.7371</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_000a6266a</td>\n      <td>trt_cp</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>0.6280</td>\n      <td>0.5817</td>\n      <td>1.5540</td>\n      <td>-0.0764</td>\n      <td>-0.0323</td>\n      <td>1.2390</td>\n      <td>...</td>\n      <td>-0.7250</td>\n      <td>-0.6297</td>\n      <td>0.6103</td>\n      <td>0.0223</td>\n      <td>-1.3240</td>\n      <td>-0.3174</td>\n      <td>-0.6417</td>\n      <td>-0.2187</td>\n      <td>-1.4080</td>\n      <td>0.6931</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_0015fd391</td>\n      <td>trt_cp</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>-0.5138</td>\n      <td>-0.2491</td>\n      <td>-0.2656</td>\n      <td>0.5288</td>\n      <td>4.0620</td>\n      <td>-0.8095</td>\n      <td>...</td>\n      <td>-2.0990</td>\n      <td>-0.6441</td>\n      <td>-5.6300</td>\n      <td>-1.3780</td>\n      <td>-0.8632</td>\n      <td>-1.2880</td>\n      <td>-1.6210</td>\n      <td>-0.8784</td>\n      <td>-0.3876</td>\n      <td>-0.8154</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_001626bd3</td>\n      <td>trt_cp</td>\n      <td>72</td>\n      <td>D2</td>\n      <td>-0.3254</td>\n      <td>-0.4009</td>\n      <td>0.9700</td>\n      <td>0.6919</td>\n      <td>1.4180</td>\n      <td>-0.8244</td>\n      <td>...</td>\n      <td>0.0042</td>\n      <td>0.0048</td>\n      <td>0.6670</td>\n      <td>1.0690</td>\n      <td>0.5523</td>\n      <td>-0.3031</td>\n      <td>0.1094</td>\n      <td>0.2885</td>\n      <td>-0.3786</td>\n      <td>0.7125</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 876 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes(include=['object']).columns","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"Index(['sig_id', 'cp_type', 'cp_dose'], dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = label_encoding(train, test, ['cp_type', 'cp_time', 'cp_dose'])","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['WHERE'] = 'train'\ntest['WHERE'] = 'test'\n\ndata = train.append(test)\ndata = data.reset_index(drop=True)\ndata","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"             sig_id  cp_type  cp_time  cp_dose     g-0     g-1     g-2  \\\n0      id_000644bb2        1        0        0  1.0620  0.5577 -0.2479   \n1      id_000779bfc        1        2        0  0.0743  0.4087  0.2991   \n2      id_000a6266a        1        1        0  0.6280  0.5817  1.5540   \n3      id_0015fd391        1        1        0 -0.5138 -0.2491 -0.2656   \n4      id_001626bd3        1        2        1 -0.3254 -0.4009  0.9700   \n...             ...      ...      ...      ...     ...     ...     ...   \n27791  id_ff7004b87        1        0        0  0.4571 -0.5743  3.3930   \n27792  id_ff925dd0d        1        0        0 -0.5885 -0.2548  2.5850   \n27793  id_ffb710450        1        2        0 -0.3985 -0.1554  0.2677   \n27794  id_ffbb869f2        1        1        1 -1.0960 -1.7750 -0.3977   \n27795  id_ffd5800b6        1        2        0 -0.5174  0.2953  0.3286   \n\n          g-3     g-4     g-5  ...    c-91    c-92    c-93    c-94    c-95  \\\n0     -0.6208 -0.1944 -1.0120  ...  0.2584  0.8076  0.5523 -0.1912  0.6584   \n1      0.0604  1.0190  0.5207  ...  0.7543  0.4708  0.0230  0.2957  0.4899   \n2     -0.0764 -0.0323  1.2390  ... -0.6297  0.6103  0.0223 -1.3240 -0.3174   \n3      0.5288  4.0620 -0.8095  ... -0.6441 -5.6300 -1.3780 -0.8632 -1.2880   \n4      0.6919  1.4180 -0.8244  ...  0.0048  0.6670  1.0690  0.5523 -0.3031   \n...       ...     ...     ...  ...     ...     ...     ...     ...     ...   \n27791 -0.6202  0.8557  1.6240  ... -0.6422 -0.4367  0.0159 -0.6539 -0.4791   \n27792  0.3456  0.4401  0.3107  ...  0.5780 -0.5888  0.8057  0.9312  1.2730   \n27793 -0.6813  0.0152  0.4791  ...  0.9153 -0.1862  0.4049  0.9568  0.4666   \n27794  1.0160 -1.3350 -0.2207  ... -0.4473 -0.8192  0.7785  0.3133  0.1286   \n27795 -0.0428 -0.0800  0.8702  ...  0.1708  0.5939 -0.0507  0.2811 -0.4041   \n\n         c-96    c-97    c-98    c-99  WHERE  \n0     -0.3981  0.2139  0.3801  0.4176  train  \n1      0.1522  0.1241  0.6077  0.7371  train  \n2     -0.6417 -0.2187 -1.4080  0.6931  train  \n3     -1.6210 -0.8784 -0.3876 -0.8154  train  \n4      0.1094  0.2885 -0.3786  0.7125  train  \n...       ...     ...     ...     ...    ...  \n27791 -1.2680 -1.1280 -0.4167 -0.6600   test  \n27792  0.2614 -0.2790 -0.0131 -0.0934   test  \n27793  0.0461  0.5888 -0.4205 -0.1504   test  \n27794 -0.2618  0.5074  0.7430 -0.0484   test  \n27795 -0.4948  0.0757 -0.1356  0.5280   test  \n\n[27796 rows x 877 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>...</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n      <th>WHERE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0620</td>\n      <td>0.5577</td>\n      <td>-0.2479</td>\n      <td>-0.6208</td>\n      <td>-0.1944</td>\n      <td>-1.0120</td>\n      <td>...</td>\n      <td>0.2584</td>\n      <td>0.8076</td>\n      <td>0.5523</td>\n      <td>-0.1912</td>\n      <td>0.6584</td>\n      <td>-0.3981</td>\n      <td>0.2139</td>\n      <td>0.3801</td>\n      <td>0.4176</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_000779bfc</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.0743</td>\n      <td>0.4087</td>\n      <td>0.2991</td>\n      <td>0.0604</td>\n      <td>1.0190</td>\n      <td>0.5207</td>\n      <td>...</td>\n      <td>0.7543</td>\n      <td>0.4708</td>\n      <td>0.0230</td>\n      <td>0.2957</td>\n      <td>0.4899</td>\n      <td>0.1522</td>\n      <td>0.1241</td>\n      <td>0.6077</td>\n      <td>0.7371</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_000a6266a</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.6280</td>\n      <td>0.5817</td>\n      <td>1.5540</td>\n      <td>-0.0764</td>\n      <td>-0.0323</td>\n      <td>1.2390</td>\n      <td>...</td>\n      <td>-0.6297</td>\n      <td>0.6103</td>\n      <td>0.0223</td>\n      <td>-1.3240</td>\n      <td>-0.3174</td>\n      <td>-0.6417</td>\n      <td>-0.2187</td>\n      <td>-1.4080</td>\n      <td>0.6931</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_0015fd391</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-0.5138</td>\n      <td>-0.2491</td>\n      <td>-0.2656</td>\n      <td>0.5288</td>\n      <td>4.0620</td>\n      <td>-0.8095</td>\n      <td>...</td>\n      <td>-0.6441</td>\n      <td>-5.6300</td>\n      <td>-1.3780</td>\n      <td>-0.8632</td>\n      <td>-1.2880</td>\n      <td>-1.6210</td>\n      <td>-0.8784</td>\n      <td>-0.3876</td>\n      <td>-0.8154</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_001626bd3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>-0.3254</td>\n      <td>-0.4009</td>\n      <td>0.9700</td>\n      <td>0.6919</td>\n      <td>1.4180</td>\n      <td>-0.8244</td>\n      <td>...</td>\n      <td>0.0048</td>\n      <td>0.6670</td>\n      <td>1.0690</td>\n      <td>0.5523</td>\n      <td>-0.3031</td>\n      <td>0.1094</td>\n      <td>0.2885</td>\n      <td>-0.3786</td>\n      <td>0.7125</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27791</th>\n      <td>id_ff7004b87</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.4571</td>\n      <td>-0.5743</td>\n      <td>3.3930</td>\n      <td>-0.6202</td>\n      <td>0.8557</td>\n      <td>1.6240</td>\n      <td>...</td>\n      <td>-0.6422</td>\n      <td>-0.4367</td>\n      <td>0.0159</td>\n      <td>-0.6539</td>\n      <td>-0.4791</td>\n      <td>-1.2680</td>\n      <td>-1.1280</td>\n      <td>-0.4167</td>\n      <td>-0.6600</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>27792</th>\n      <td>id_ff925dd0d</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.5885</td>\n      <td>-0.2548</td>\n      <td>2.5850</td>\n      <td>0.3456</td>\n      <td>0.4401</td>\n      <td>0.3107</td>\n      <td>...</td>\n      <td>0.5780</td>\n      <td>-0.5888</td>\n      <td>0.8057</td>\n      <td>0.9312</td>\n      <td>1.2730</td>\n      <td>0.2614</td>\n      <td>-0.2790</td>\n      <td>-0.0131</td>\n      <td>-0.0934</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>27793</th>\n      <td>id_ffb710450</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>-0.3985</td>\n      <td>-0.1554</td>\n      <td>0.2677</td>\n      <td>-0.6813</td>\n      <td>0.0152</td>\n      <td>0.4791</td>\n      <td>...</td>\n      <td>0.9153</td>\n      <td>-0.1862</td>\n      <td>0.4049</td>\n      <td>0.9568</td>\n      <td>0.4666</td>\n      <td>0.0461</td>\n      <td>0.5888</td>\n      <td>-0.4205</td>\n      <td>-0.1504</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>27794</th>\n      <td>id_ffbb869f2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0960</td>\n      <td>-1.7750</td>\n      <td>-0.3977</td>\n      <td>1.0160</td>\n      <td>-1.3350</td>\n      <td>-0.2207</td>\n      <td>...</td>\n      <td>-0.4473</td>\n      <td>-0.8192</td>\n      <td>0.7785</td>\n      <td>0.3133</td>\n      <td>0.1286</td>\n      <td>-0.2618</td>\n      <td>0.5074</td>\n      <td>0.7430</td>\n      <td>-0.0484</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>27795</th>\n      <td>id_ffd5800b6</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>-0.5174</td>\n      <td>0.2953</td>\n      <td>0.3286</td>\n      <td>-0.0428</td>\n      <td>-0.0800</td>\n      <td>0.8702</td>\n      <td>...</td>\n      <td>0.1708</td>\n      <td>0.5939</td>\n      <td>-0.0507</td>\n      <td>0.2811</td>\n      <td>-0.4041</td>\n      <td>-0.4948</td>\n      <td>0.0757</td>\n      <td>-0.1356</td>\n      <td>0.5280</td>\n      <td>test</td>\n    </tr>\n  </tbody>\n</table>\n<p>27796 rows × 877 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select control data\nctl = train[(train.cp_type==0)].copy()\nctl = ctl.reset_index(drop=True)\nctl","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"            sig_id  cp_type  cp_time  cp_dose     g-0     g-1     g-2     g-3  \\\n0     id_0054388ec        0        1        0 -0.6696 -0.2718 -1.2230 -0.6226   \n1     id_0079af0fb        0        0        0 -0.1636 -1.8230 -0.5211  0.3054   \n2     id_007bfbb91        0        0        1 -1.3200 -1.7340 -0.0741  1.5440   \n3     id_008a986b7        0        1        1  0.4860  0.1430  0.5281 -0.0022   \n4     id_009d8340f        0        0        1  0.4471 -0.0839 -0.3864  0.3196   \n...            ...      ...      ...      ...     ...     ...     ...     ...   \n1861  id_ff1f9e5fc        0        0        0  0.0465  0.6430 -0.2453  0.3521   \n1862  id_ff469c482        0        2        0 -0.6204 -1.3800 -1.1880 -0.6573   \n1863  id_ff89331ca        0        2        1 -0.1051  0.4335  0.3648 -0.0525   \n1864  id_ffd26f361        0        1        1  0.6008  0.2781 -0.3319 -0.8782   \n1865  id_fffc1c3f4        0        1        1  0.3942  0.3756  0.3109 -0.7389   \n\n         g-4     g-5  ...    c-91    c-92    c-93    c-94    c-95    c-96  \\\n0    -0.7220  0.1588  ...  0.6405  0.5429  0.3562  1.3290  0.5573  0.8837   \n1    -1.1280  0.6041  ... -0.6828 -0.6594 -0.2043  0.3571 -0.1319  0.2187   \n2    -1.8290 -0.0951  ...  1.0340  0.7393  1.1020  0.3786  0.2636 -0.5751   \n3    -0.2334 -0.6607  ... -0.1866  0.7629  0.3288 -0.9915 -0.3694 -0.4979   \n4     0.2584 -0.3156  ...  0.3939  0.7665  0.7932 -0.6804 -0.0435  0.0949   \n...      ...     ...  ...     ...     ...     ...     ...     ...     ...   \n1861  0.6195 -0.6659  ...  0.1621 -0.2065  0.8314 -0.1891 -0.0074  0.2131   \n1862 -0.8408 -0.2447  ...  0.7366  0.5468  1.1450  0.5254  0.6224 -0.9630   \n1863 -0.3632 -0.3228  ...  0.4885  0.4963  0.9434 -0.4779  0.6951  0.9517   \n1864  0.9281  0.7535  ...  0.9569 -0.2065 -0.4918  0.7863  0.0504  0.8813   \n1865  0.5505 -0.0159  ...  0.3755  0.7343  0.2807  0.4116  0.6422  0.2256   \n\n        c-97    c-98    c-99  WHERE  \n0     0.5534  0.8976  1.0050  train  \n1     0.0737  0.6498 -1.4820  train  \n2     0.3362  0.8543  0.9180  train  \n3     0.5281 -0.9245 -0.9367  train  \n4     0.2000 -0.4326  0.9364  train  \n...      ...     ...     ...    ...  \n1861 -0.1419 -0.2422  0.0457  train  \n1862  0.8872 -0.0742 -0.6777  train  \n1863 -0.8110 -0.1748 -0.3626  train  \n1864  0.7757 -0.5272  0.7082  train  \n1865  0.7592  0.6656  0.3808  train  \n\n[1866 rows x 877 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>...</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n      <th>WHERE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_0054388ec</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-0.6696</td>\n      <td>-0.2718</td>\n      <td>-1.2230</td>\n      <td>-0.6226</td>\n      <td>-0.7220</td>\n      <td>0.1588</td>\n      <td>...</td>\n      <td>0.6405</td>\n      <td>0.5429</td>\n      <td>0.3562</td>\n      <td>1.3290</td>\n      <td>0.5573</td>\n      <td>0.8837</td>\n      <td>0.5534</td>\n      <td>0.8976</td>\n      <td>1.0050</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_0079af0fb</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.1636</td>\n      <td>-1.8230</td>\n      <td>-0.5211</td>\n      <td>0.3054</td>\n      <td>-1.1280</td>\n      <td>0.6041</td>\n      <td>...</td>\n      <td>-0.6828</td>\n      <td>-0.6594</td>\n      <td>-0.2043</td>\n      <td>0.3571</td>\n      <td>-0.1319</td>\n      <td>0.2187</td>\n      <td>0.0737</td>\n      <td>0.6498</td>\n      <td>-1.4820</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_007bfbb91</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-1.3200</td>\n      <td>-1.7340</td>\n      <td>-0.0741</td>\n      <td>1.5440</td>\n      <td>-1.8290</td>\n      <td>-0.0951</td>\n      <td>...</td>\n      <td>1.0340</td>\n      <td>0.7393</td>\n      <td>1.1020</td>\n      <td>0.3786</td>\n      <td>0.2636</td>\n      <td>-0.5751</td>\n      <td>0.3362</td>\n      <td>0.8543</td>\n      <td>0.9180</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_008a986b7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.4860</td>\n      <td>0.1430</td>\n      <td>0.5281</td>\n      <td>-0.0022</td>\n      <td>-0.2334</td>\n      <td>-0.6607</td>\n      <td>...</td>\n      <td>-0.1866</td>\n      <td>0.7629</td>\n      <td>0.3288</td>\n      <td>-0.9915</td>\n      <td>-0.3694</td>\n      <td>-0.4979</td>\n      <td>0.5281</td>\n      <td>-0.9245</td>\n      <td>-0.9367</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_009d8340f</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.4471</td>\n      <td>-0.0839</td>\n      <td>-0.3864</td>\n      <td>0.3196</td>\n      <td>0.2584</td>\n      <td>-0.3156</td>\n      <td>...</td>\n      <td>0.3939</td>\n      <td>0.7665</td>\n      <td>0.7932</td>\n      <td>-0.6804</td>\n      <td>-0.0435</td>\n      <td>0.0949</td>\n      <td>0.2000</td>\n      <td>-0.4326</td>\n      <td>0.9364</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1861</th>\n      <td>id_ff1f9e5fc</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0465</td>\n      <td>0.6430</td>\n      <td>-0.2453</td>\n      <td>0.3521</td>\n      <td>0.6195</td>\n      <td>-0.6659</td>\n      <td>...</td>\n      <td>0.1621</td>\n      <td>-0.2065</td>\n      <td>0.8314</td>\n      <td>-0.1891</td>\n      <td>-0.0074</td>\n      <td>0.2131</td>\n      <td>-0.1419</td>\n      <td>-0.2422</td>\n      <td>0.0457</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1862</th>\n      <td>id_ff469c482</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>-0.6204</td>\n      <td>-1.3800</td>\n      <td>-1.1880</td>\n      <td>-0.6573</td>\n      <td>-0.8408</td>\n      <td>-0.2447</td>\n      <td>...</td>\n      <td>0.7366</td>\n      <td>0.5468</td>\n      <td>1.1450</td>\n      <td>0.5254</td>\n      <td>0.6224</td>\n      <td>-0.9630</td>\n      <td>0.8872</td>\n      <td>-0.0742</td>\n      <td>-0.6777</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1863</th>\n      <td>id_ff89331ca</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>-0.1051</td>\n      <td>0.4335</td>\n      <td>0.3648</td>\n      <td>-0.0525</td>\n      <td>-0.3632</td>\n      <td>-0.3228</td>\n      <td>...</td>\n      <td>0.4885</td>\n      <td>0.4963</td>\n      <td>0.9434</td>\n      <td>-0.4779</td>\n      <td>0.6951</td>\n      <td>0.9517</td>\n      <td>-0.8110</td>\n      <td>-0.1748</td>\n      <td>-0.3626</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1864</th>\n      <td>id_ffd26f361</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.6008</td>\n      <td>0.2781</td>\n      <td>-0.3319</td>\n      <td>-0.8782</td>\n      <td>0.9281</td>\n      <td>0.7535</td>\n      <td>...</td>\n      <td>0.9569</td>\n      <td>-0.2065</td>\n      <td>-0.4918</td>\n      <td>0.7863</td>\n      <td>0.0504</td>\n      <td>0.8813</td>\n      <td>0.7757</td>\n      <td>-0.5272</td>\n      <td>0.7082</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1865</th>\n      <td>id_fffc1c3f4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.3942</td>\n      <td>0.3756</td>\n      <td>0.3109</td>\n      <td>-0.7389</td>\n      <td>0.5505</td>\n      <td>-0.0159</td>\n      <td>...</td>\n      <td>0.3755</td>\n      <td>0.7343</td>\n      <td>0.2807</td>\n      <td>0.4116</td>\n      <td>0.6422</td>\n      <td>0.2256</td>\n      <td>0.7592</td>\n      <td>0.6656</td>\n      <td>0.3808</td>\n      <td>train</td>\n    </tr>\n  </tbody>\n</table>\n<p>1866 rows × 877 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clipping\n\ndef outlaier_clip(df):\n    df = df.copy()\n    clipping = df.columns[4:6]\n    for col in clipping:\n        lower, upper= np.percentile(df[col], [10, 90])\n        df[col] = np.clip(df[col], lower, upper)\n    \n    return df\n\nctl_df = pd.DataFrame(columns=train.columns)\nfor i in ctl.cp_time.unique():\n    for j in ctl.cp_dose.unique():\n        print(len(ctl[(ctl.cp_time==i) & (ctl.cp_dose==j)]))\n        tmp_ctl = ctl[(ctl.cp_time==i) & (ctl.cp_dose==j)]\n        tmp_ctl = outlaier_clip(tmp_ctl)\n        ctl_df = pd.concat([ctl_df, tmp_ctl], axis=0).reset_index(drop=True)\nctl_df","execution_count":24,"outputs":[{"output_type":"stream","text":"343\n305\n301\n305\n307\n305\n","name":"stdout"},{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"            sig_id cp_type cp_time cp_dose     g-0      g-1     g-2     g-3  \\\n0     id_0054388ec       0       1       0 -0.6696 -0.27180 -1.2230 -0.6226   \n1     id_01b05de6c       0       1       0 -0.3176  0.48512 -0.3773  0.5555   \n2     id_020ba48d9       0       1       0  0.8094 -0.33390  1.1130 -0.4192   \n3     id_02c93b4a5       0       1       0 -0.0884 -1.54100  0.5562  0.4661   \n4     id_03bc6d412       0       1       0  0.8431  0.09310 -0.0958 -0.4337   \n...            ...     ...     ...     ...     ...      ...     ...     ...   \n1861  id_fbb620181       0       2       1 -0.3490  0.72970 -4.1480 -1.0030   \n1862  id_fbba92030       0       2       1 -0.8921  1.13520  0.4227  1.3270   \n1863  id_fbc0849ad       0       2       1 -0.2948  0.21960 -0.3252 -0.2084   \n1864  id_fd93fa37d       0       2       1 -0.2984 -0.47880 -2.3560 -0.6848   \n1865  id_ff89331ca       0       2       1 -0.1051  0.43350  0.3648 -0.0525   \n\n         g-4     g-5  ...    c-91    c-92    c-93    c-94    c-95    c-96  \\\n0    -0.7220  0.1588  ...  0.6405  0.5429  0.3562  1.3290  0.5573  0.8837   \n1    -0.4877  0.3413  ... -0.6163  0.8001  0.3389  0.7437  0.7533  0.2406   \n2     0.4545 -0.4771  ... -0.4414 -0.2442  0.0557 -0.6597 -0.3713 -0.6929   \n3    -0.1683  0.9515  ...  0.4766  0.9697  0.4613  0.8957  0.8227  1.2300   \n4     1.0690 -0.6440  ... -1.1510 -0.5010 -0.2736  0.1270  0.3511  0.1233   \n...      ...     ...  ...     ...     ...     ...     ...     ...     ...   \n1861  2.4070 -0.9784  ... -1.4260 -0.8613 -0.0243 -0.6109  0.6377 -1.2590   \n1862 -0.9839 -0.8469  ...  1.1340  0.7007  1.3320  0.6622  0.6167  0.9459   \n1863 -0.6518  0.3542  ...  0.5014  0.6009  0.4560 -0.3360 -0.5091  0.6569   \n1864  1.8700  1.0240  ... -0.4422 -0.6665 -0.8033 -1.0520  0.4103  0.4462   \n1865 -0.3632 -0.3228  ...  0.4885  0.4963  0.9434 -0.4779  0.6951  0.9517   \n\n        c-97    c-98    c-99  WHERE  \n0     0.5534  0.8976  1.0050  train  \n1     0.5057  0.8017  1.0280  train  \n2    -0.3258  0.3602 -0.0595  train  \n3     1.7180  0.6542  0.0580  train  \n4    -0.8520  0.1767  0.1153  train  \n...      ...     ...     ...    ...  \n1861 -0.7238 -0.7734 -0.1274  train  \n1862 -0.0344 -0.0186  0.5173  train  \n1863  0.6203  0.6764  0.4504  train  \n1864 -1.1580 -1.3060  0.1925  train  \n1865 -0.8110 -0.1748 -0.3626  train  \n\n[1866 rows x 877 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>...</th>\n      <th>c-91</th>\n      <th>c-92</th>\n      <th>c-93</th>\n      <th>c-94</th>\n      <th>c-95</th>\n      <th>c-96</th>\n      <th>c-97</th>\n      <th>c-98</th>\n      <th>c-99</th>\n      <th>WHERE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_0054388ec</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-0.6696</td>\n      <td>-0.27180</td>\n      <td>-1.2230</td>\n      <td>-0.6226</td>\n      <td>-0.7220</td>\n      <td>0.1588</td>\n      <td>...</td>\n      <td>0.6405</td>\n      <td>0.5429</td>\n      <td>0.3562</td>\n      <td>1.3290</td>\n      <td>0.5573</td>\n      <td>0.8837</td>\n      <td>0.5534</td>\n      <td>0.8976</td>\n      <td>1.0050</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_01b05de6c</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-0.3176</td>\n      <td>0.48512</td>\n      <td>-0.3773</td>\n      <td>0.5555</td>\n      <td>-0.4877</td>\n      <td>0.3413</td>\n      <td>...</td>\n      <td>-0.6163</td>\n      <td>0.8001</td>\n      <td>0.3389</td>\n      <td>0.7437</td>\n      <td>0.7533</td>\n      <td>0.2406</td>\n      <td>0.5057</td>\n      <td>0.8017</td>\n      <td>1.0280</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_020ba48d9</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.8094</td>\n      <td>-0.33390</td>\n      <td>1.1130</td>\n      <td>-0.4192</td>\n      <td>0.4545</td>\n      <td>-0.4771</td>\n      <td>...</td>\n      <td>-0.4414</td>\n      <td>-0.2442</td>\n      <td>0.0557</td>\n      <td>-0.6597</td>\n      <td>-0.3713</td>\n      <td>-0.6929</td>\n      <td>-0.3258</td>\n      <td>0.3602</td>\n      <td>-0.0595</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_02c93b4a5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>-0.0884</td>\n      <td>-1.54100</td>\n      <td>0.5562</td>\n      <td>0.4661</td>\n      <td>-0.1683</td>\n      <td>0.9515</td>\n      <td>...</td>\n      <td>0.4766</td>\n      <td>0.9697</td>\n      <td>0.4613</td>\n      <td>0.8957</td>\n      <td>0.8227</td>\n      <td>1.2300</td>\n      <td>1.7180</td>\n      <td>0.6542</td>\n      <td>0.0580</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_03bc6d412</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.8431</td>\n      <td>0.09310</td>\n      <td>-0.0958</td>\n      <td>-0.4337</td>\n      <td>1.0690</td>\n      <td>-0.6440</td>\n      <td>...</td>\n      <td>-1.1510</td>\n      <td>-0.5010</td>\n      <td>-0.2736</td>\n      <td>0.1270</td>\n      <td>0.3511</td>\n      <td>0.1233</td>\n      <td>-0.8520</td>\n      <td>0.1767</td>\n      <td>0.1153</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1861</th>\n      <td>id_fbb620181</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>-0.3490</td>\n      <td>0.72970</td>\n      <td>-4.1480</td>\n      <td>-1.0030</td>\n      <td>2.4070</td>\n      <td>-0.9784</td>\n      <td>...</td>\n      <td>-1.4260</td>\n      <td>-0.8613</td>\n      <td>-0.0243</td>\n      <td>-0.6109</td>\n      <td>0.6377</td>\n      <td>-1.2590</td>\n      <td>-0.7238</td>\n      <td>-0.7734</td>\n      <td>-0.1274</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1862</th>\n      <td>id_fbba92030</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>-0.8921</td>\n      <td>1.13520</td>\n      <td>0.4227</td>\n      <td>1.3270</td>\n      <td>-0.9839</td>\n      <td>-0.8469</td>\n      <td>...</td>\n      <td>1.1340</td>\n      <td>0.7007</td>\n      <td>1.3320</td>\n      <td>0.6622</td>\n      <td>0.6167</td>\n      <td>0.9459</td>\n      <td>-0.0344</td>\n      <td>-0.0186</td>\n      <td>0.5173</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1863</th>\n      <td>id_fbc0849ad</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>-0.2948</td>\n      <td>0.21960</td>\n      <td>-0.3252</td>\n      <td>-0.2084</td>\n      <td>-0.6518</td>\n      <td>0.3542</td>\n      <td>...</td>\n      <td>0.5014</td>\n      <td>0.6009</td>\n      <td>0.4560</td>\n      <td>-0.3360</td>\n      <td>-0.5091</td>\n      <td>0.6569</td>\n      <td>0.6203</td>\n      <td>0.6764</td>\n      <td>0.4504</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1864</th>\n      <td>id_fd93fa37d</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>-0.2984</td>\n      <td>-0.47880</td>\n      <td>-2.3560</td>\n      <td>-0.6848</td>\n      <td>1.8700</td>\n      <td>1.0240</td>\n      <td>...</td>\n      <td>-0.4422</td>\n      <td>-0.6665</td>\n      <td>-0.8033</td>\n      <td>-1.0520</td>\n      <td>0.4103</td>\n      <td>0.4462</td>\n      <td>-1.1580</td>\n      <td>-1.3060</td>\n      <td>0.1925</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1865</th>\n      <td>id_ff89331ca</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>-0.1051</td>\n      <td>0.43350</td>\n      <td>0.3648</td>\n      <td>-0.0525</td>\n      <td>-0.3632</td>\n      <td>-0.3228</td>\n      <td>...</td>\n      <td>0.4885</td>\n      <td>0.4963</td>\n      <td>0.9434</td>\n      <td>-0.4779</td>\n      <td>0.6951</td>\n      <td>0.9517</td>\n      <td>-0.8110</td>\n      <td>-0.1748</td>\n      <td>-0.3626</td>\n      <td>train</td>\n    </tr>\n  </tbody>\n</table>\n<p>1866 rows × 877 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_list = list(data.columns)[:-1]\ndata_df = pd.DataFrame(columns=col_list)\nSplitdata = []\nd = 1e-6\n\nfor i in tqdm(data.cp_time.unique()):\n    for j in data.cp_dose.unique():\n        select = data[(data.cp_time==i) & (data.cp_dose==j)]\n        print(len(select))\n        \n        for k in list(select['WHERE']): Splitdata.append(k)\n        \n        select = select.drop(columns='WHERE')\n        med = ctl[(ctl.cp_time==i) & (ctl.cp_dose==j)].iloc[:, 4:].median()\n        \n        f_div = lambda x: ((x+d)*10 / (abs(med)+d))**3\n        select_div = select.iloc[:,4:].apply(f_div, axis=1).add_prefix('d_')\n        tmp_data = pd.concat([select, select_div], axis=1, sort=False)\n        \n        \n        f_diff = lambda x: ((x-med)*10)**2\n        select_diff = select.iloc[:,4:].apply(f_diff, axis=1).add_prefix('df_')\n        tmp_data = pd.concat([tmp_data, select_diff], axis=1, sort=False)\n        \n        data_df = pd.concat([data_df, tmp_data], axis=0)\n        \ndata_df","execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"451dc299fe5248079b13414b396f63c9"}},"metadata":{}},{"output_type":"stream","text":"4534\n4538\n4561\n4536\n5079\n4548\n\n","name":"stdout"},{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"             sig_id cp_type cp_time cp_dose     g-0     g-1     g-2     g-3  \\\n0      id_000644bb2       1       0       0  1.0620  0.5577 -0.2479 -0.6208   \n5      id_001762a82       1       0       0 -0.6111  0.2941 -0.9901  0.2277   \n36     id_00762e877       1       0       0 -0.4026  0.1662 -0.6307 -0.4438   \n38     id_0079af0fb       0       0       0 -0.1636 -1.8230 -0.5211  0.3054   \n39     id_0079d45d3       1       0       0  1.6310 -2.1430 -0.0943 -1.1990   \n...             ...     ...     ...     ...     ...     ...     ...     ...   \n27754  id_fca887f42       1       1       1 -0.4157 -0.0461 -0.9751  0.7702   \n27759  id_fce497048       1       1       1  0.7107 -0.3274 -0.0099 -1.4950   \n27768  id_fd75349b2       1       1       1 -0.0776  0.4437 -0.1530  1.2300   \n27784  id_fed0f2fe0       1       1       1 -1.0740  0.7350  0.3304 -0.6764   \n27794  id_ffbb869f2       1       1       1 -1.0960 -1.7750 -0.3977  1.0160   \n\n          g-4     g-5  ...      df_c-90      df_c-91      df_c-92  \\\n0     -0.1944 -1.0120  ...     0.099225     0.289444    38.626225   \n5      1.2810  0.5203  ...   231.435369    71.368704   143.544361   \n36    -0.5992 -0.2523  ...    90.897156    22.667121     7.360369   \n38    -1.1280  0.6041  ...    49.801249    99.002500    71.487025   \n39     0.4869 -0.0935  ...  1050.861889  3701.748964   450.330841   \n...       ...     ...  ...          ...          ...          ...   \n27754 -0.1861  0.3608  ...    18.974736    70.711281     1.157776   \n27759  0.6673 -1.5380  ...   199.430884    72.539289  1238.336100   \n27768 -0.5804  0.0633  ...     0.722500     0.222784    20.007729   \n27784  0.1435 -1.6810  ...   135.117376    64.192144     6.990736   \n27794 -1.3350 -0.2207  ...     1.530169    43.626025    72.454144   \n\n           df_c-93      df_c-94     df_c-95     df_c-96     df_c-97  \\\n0         1.731856    43.046721   10.086976   59.861169    1.567504   \n5       219.128809    92.563641    1.234321   69.956496    0.139876   \n36        7.667361    20.848356   16.777216    2.762244   25.160256   \n38       39.062500     1.162084   22.344529    2.461761    7.043716   \n39     1093.426489  1045.164241  797.949504  947.162176  671.898241   \n...            ...          ...         ...         ...         ...   \n27754    24.830289    10.784656   35.640900    6.702921   49.942489   \n27759   106.357969  3121.009956  194.435136  651.168324  170.485249   \n27768    47.444544    46.090521  100.220121    3.101121    0.405769   \n27784   124.478649    76.160529  290.838916  272.514064  151.659225   \n27794    28.793956     0.157609    0.937024   16.777216    4.439449   \n\n           df_c-98     df_c-99  \n0         0.147456    0.216225  \n5        19.633761  105.657841  \n36        0.050176    1.223236  \n38        9.492561  343.397961  \n39     2680.857729  103.469584  \n...            ...         ...  \n27754     0.300304   60.497284  \n27759   759.443364  248.755984  \n27768   616.429584   43.086096  \n27784   177.102864  207.994084  \n27794    20.903184    6.533136  \n\n[27796 rows x 2620 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>...</th>\n      <th>df_c-90</th>\n      <th>df_c-91</th>\n      <th>df_c-92</th>\n      <th>df_c-93</th>\n      <th>df_c-94</th>\n      <th>df_c-95</th>\n      <th>df_c-96</th>\n      <th>df_c-97</th>\n      <th>df_c-98</th>\n      <th>df_c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0620</td>\n      <td>0.5577</td>\n      <td>-0.2479</td>\n      <td>-0.6208</td>\n      <td>-0.1944</td>\n      <td>-1.0120</td>\n      <td>...</td>\n      <td>0.099225</td>\n      <td>0.289444</td>\n      <td>38.626225</td>\n      <td>1.731856</td>\n      <td>43.046721</td>\n      <td>10.086976</td>\n      <td>59.861169</td>\n      <td>1.567504</td>\n      <td>0.147456</td>\n      <td>0.216225</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>id_001762a82</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.6111</td>\n      <td>0.2941</td>\n      <td>-0.9901</td>\n      <td>0.2277</td>\n      <td>1.2810</td>\n      <td>0.5203</td>\n      <td>...</td>\n      <td>231.435369</td>\n      <td>71.368704</td>\n      <td>143.544361</td>\n      <td>219.128809</td>\n      <td>92.563641</td>\n      <td>1.234321</td>\n      <td>69.956496</td>\n      <td>0.139876</td>\n      <td>19.633761</td>\n      <td>105.657841</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>id_00762e877</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.4026</td>\n      <td>0.1662</td>\n      <td>-0.6307</td>\n      <td>-0.4438</td>\n      <td>-0.5992</td>\n      <td>-0.2523</td>\n      <td>...</td>\n      <td>90.897156</td>\n      <td>22.667121</td>\n      <td>7.360369</td>\n      <td>7.667361</td>\n      <td>20.848356</td>\n      <td>16.777216</td>\n      <td>2.762244</td>\n      <td>25.160256</td>\n      <td>0.050176</td>\n      <td>1.223236</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>id_0079af0fb</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.1636</td>\n      <td>-1.8230</td>\n      <td>-0.5211</td>\n      <td>0.3054</td>\n      <td>-1.1280</td>\n      <td>0.6041</td>\n      <td>...</td>\n      <td>49.801249</td>\n      <td>99.002500</td>\n      <td>71.487025</td>\n      <td>39.062500</td>\n      <td>1.162084</td>\n      <td>22.344529</td>\n      <td>2.461761</td>\n      <td>7.043716</td>\n      <td>9.492561</td>\n      <td>343.397961</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>id_0079d45d3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.6310</td>\n      <td>-2.1430</td>\n      <td>-0.0943</td>\n      <td>-1.1990</td>\n      <td>0.4869</td>\n      <td>-0.0935</td>\n      <td>...</td>\n      <td>1050.861889</td>\n      <td>3701.748964</td>\n      <td>450.330841</td>\n      <td>1093.426489</td>\n      <td>1045.164241</td>\n      <td>797.949504</td>\n      <td>947.162176</td>\n      <td>671.898241</td>\n      <td>2680.857729</td>\n      <td>103.469584</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27754</th>\n      <td>id_fca887f42</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-0.4157</td>\n      <td>-0.0461</td>\n      <td>-0.9751</td>\n      <td>0.7702</td>\n      <td>-0.1861</td>\n      <td>0.3608</td>\n      <td>...</td>\n      <td>18.974736</td>\n      <td>70.711281</td>\n      <td>1.157776</td>\n      <td>24.830289</td>\n      <td>10.784656</td>\n      <td>35.640900</td>\n      <td>6.702921</td>\n      <td>49.942489</td>\n      <td>0.300304</td>\n      <td>60.497284</td>\n    </tr>\n    <tr>\n      <th>27759</th>\n      <td>id_fce497048</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.7107</td>\n      <td>-0.3274</td>\n      <td>-0.0099</td>\n      <td>-1.4950</td>\n      <td>0.6673</td>\n      <td>-1.5380</td>\n      <td>...</td>\n      <td>199.430884</td>\n      <td>72.539289</td>\n      <td>1238.336100</td>\n      <td>106.357969</td>\n      <td>3121.009956</td>\n      <td>194.435136</td>\n      <td>651.168324</td>\n      <td>170.485249</td>\n      <td>759.443364</td>\n      <td>248.755984</td>\n    </tr>\n    <tr>\n      <th>27768</th>\n      <td>id_fd75349b2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-0.0776</td>\n      <td>0.4437</td>\n      <td>-0.1530</td>\n      <td>1.2300</td>\n      <td>-0.5804</td>\n      <td>0.0633</td>\n      <td>...</td>\n      <td>0.722500</td>\n      <td>0.222784</td>\n      <td>20.007729</td>\n      <td>47.444544</td>\n      <td>46.090521</td>\n      <td>100.220121</td>\n      <td>3.101121</td>\n      <td>0.405769</td>\n      <td>616.429584</td>\n      <td>43.086096</td>\n    </tr>\n    <tr>\n      <th>27784</th>\n      <td>id_fed0f2fe0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0740</td>\n      <td>0.7350</td>\n      <td>0.3304</td>\n      <td>-0.6764</td>\n      <td>0.1435</td>\n      <td>-1.6810</td>\n      <td>...</td>\n      <td>135.117376</td>\n      <td>64.192144</td>\n      <td>6.990736</td>\n      <td>124.478649</td>\n      <td>76.160529</td>\n      <td>290.838916</td>\n      <td>272.514064</td>\n      <td>151.659225</td>\n      <td>177.102864</td>\n      <td>207.994084</td>\n    </tr>\n    <tr>\n      <th>27794</th>\n      <td>id_ffbb869f2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0960</td>\n      <td>-1.7750</td>\n      <td>-0.3977</td>\n      <td>1.0160</td>\n      <td>-1.3350</td>\n      <td>-0.2207</td>\n      <td>...</td>\n      <td>1.530169</td>\n      <td>43.626025</td>\n      <td>72.454144</td>\n      <td>28.793956</td>\n      <td>0.157609</td>\n      <td>0.937024</td>\n      <td>16.777216</td>\n      <td>4.439449</td>\n      <td>20.903184</td>\n      <td>6.533136</td>\n    </tr>\n  </tbody>\n</table>\n<p>27796 rows × 2620 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clipping\nclipping = data_df.columns[4:]\nfor col in tqdm(clipping):\n    lower, upper = np.percentile(data_df[col], [1, 99])\n    data_df[col] = np.clip(data_df[col], lower, upper)\ndata_df","execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=2616.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ede083abe5dc41f3a1fc90cc5012a534"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"             sig_id cp_type cp_time cp_dose     g-0     g-1     g-2     g-3  \\\n0      id_000644bb2       1       0       0  1.0620  0.5577 -0.2479 -0.6208   \n5      id_001762a82       1       0       0 -0.6111  0.2941 -0.9901  0.2277   \n36     id_00762e877       1       0       0 -0.4026  0.1662 -0.6307 -0.4438   \n38     id_0079af0fb       0       0       0 -0.1636 -1.8230 -0.5211  0.3054   \n39     id_0079d45d3       1       0       0  1.6310 -2.1430 -0.0943 -1.1990   \n...             ...     ...     ...     ...     ...     ...     ...     ...   \n27754  id_fca887f42       1       1       1 -0.4157 -0.0461 -0.9751  0.7702   \n27759  id_fce497048       1       1       1  0.7107 -0.3274 -0.0099 -1.4950   \n27768  id_fd75349b2       1       1       1 -0.0776  0.4437 -0.1530  1.2300   \n27784  id_fed0f2fe0       1       1       1 -1.0740  0.7350  0.3304 -0.6764   \n27794  id_ffbb869f2       1       1       1 -1.0960 -1.7750 -0.3977  1.0160   \n\n          g-4     g-5  ...      df_c-90      df_c-91      df_c-92  \\\n0     -0.1944 -1.0120  ...     0.099225     0.289444    38.626225   \n5      1.2810  0.5203  ...   231.435369    71.368704   143.544361   \n36    -0.5992 -0.2523  ...    90.897156    22.667121     7.360369   \n38    -1.1280  0.6041  ...    49.801249    99.002500    71.487025   \n39     0.4869 -0.0935  ...  1050.861889  3701.748964   450.330841   \n...       ...     ...  ...          ...          ...          ...   \n27754 -0.1861  0.3608  ...    18.974736    70.711281     1.157776   \n27759  0.6673 -1.5380  ...   199.430884    72.539289  1238.336100   \n27768 -0.5804  0.0633  ...     0.722500     0.222784    20.007729   \n27784  0.1435 -1.6810  ...   135.117376    64.192144     6.990736   \n27794 -1.3350 -0.2207  ...     1.530169    43.626025    72.454144   \n\n           df_c-93      df_c-94     df_c-95     df_c-96     df_c-97  \\\n0         1.731856    43.046721   10.086976   59.861169    1.567504   \n5       219.128809    92.563641    1.234321   69.956496    0.139876   \n36        7.667361    20.848356   16.777216    2.762244   25.160256   \n38       39.062500     1.162084   22.344529    2.461761    7.043716   \n39     1093.426489  1045.164241  797.949504  947.162176  671.898241   \n...            ...          ...         ...         ...         ...   \n27754    24.830289    10.784656   35.640900    6.702921   49.942489   \n27759   106.357969  3121.009956  194.435136  651.168324  170.485249   \n27768    47.444544    46.090521  100.220121    3.101121    0.405769   \n27784   124.478649    76.160529  290.838916  272.514064  151.659225   \n27794    28.793956     0.157609    0.937024   16.777216    4.439449   \n\n           df_c-98     df_c-99  \n0         0.147456    0.216225  \n5        19.633761  105.657841  \n36        0.050176    1.223236  \n38        9.492561  343.397961  \n39     2680.857729  103.469584  \n...            ...         ...  \n27754     0.300304   60.497284  \n27759   759.443364  248.755984  \n27768   616.429584   43.086096  \n27784   177.102864  207.994084  \n27794    20.903184    6.533136  \n\n[27796 rows x 2620 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>...</th>\n      <th>df_c-90</th>\n      <th>df_c-91</th>\n      <th>df_c-92</th>\n      <th>df_c-93</th>\n      <th>df_c-94</th>\n      <th>df_c-95</th>\n      <th>df_c-96</th>\n      <th>df_c-97</th>\n      <th>df_c-98</th>\n      <th>df_c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0620</td>\n      <td>0.5577</td>\n      <td>-0.2479</td>\n      <td>-0.6208</td>\n      <td>-0.1944</td>\n      <td>-1.0120</td>\n      <td>...</td>\n      <td>0.099225</td>\n      <td>0.289444</td>\n      <td>38.626225</td>\n      <td>1.731856</td>\n      <td>43.046721</td>\n      <td>10.086976</td>\n      <td>59.861169</td>\n      <td>1.567504</td>\n      <td>0.147456</td>\n      <td>0.216225</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>id_001762a82</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.6111</td>\n      <td>0.2941</td>\n      <td>-0.9901</td>\n      <td>0.2277</td>\n      <td>1.2810</td>\n      <td>0.5203</td>\n      <td>...</td>\n      <td>231.435369</td>\n      <td>71.368704</td>\n      <td>143.544361</td>\n      <td>219.128809</td>\n      <td>92.563641</td>\n      <td>1.234321</td>\n      <td>69.956496</td>\n      <td>0.139876</td>\n      <td>19.633761</td>\n      <td>105.657841</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>id_00762e877</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.4026</td>\n      <td>0.1662</td>\n      <td>-0.6307</td>\n      <td>-0.4438</td>\n      <td>-0.5992</td>\n      <td>-0.2523</td>\n      <td>...</td>\n      <td>90.897156</td>\n      <td>22.667121</td>\n      <td>7.360369</td>\n      <td>7.667361</td>\n      <td>20.848356</td>\n      <td>16.777216</td>\n      <td>2.762244</td>\n      <td>25.160256</td>\n      <td>0.050176</td>\n      <td>1.223236</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>id_0079af0fb</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.1636</td>\n      <td>-1.8230</td>\n      <td>-0.5211</td>\n      <td>0.3054</td>\n      <td>-1.1280</td>\n      <td>0.6041</td>\n      <td>...</td>\n      <td>49.801249</td>\n      <td>99.002500</td>\n      <td>71.487025</td>\n      <td>39.062500</td>\n      <td>1.162084</td>\n      <td>22.344529</td>\n      <td>2.461761</td>\n      <td>7.043716</td>\n      <td>9.492561</td>\n      <td>343.397961</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>id_0079d45d3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.6310</td>\n      <td>-2.1430</td>\n      <td>-0.0943</td>\n      <td>-1.1990</td>\n      <td>0.4869</td>\n      <td>-0.0935</td>\n      <td>...</td>\n      <td>1050.861889</td>\n      <td>3701.748964</td>\n      <td>450.330841</td>\n      <td>1093.426489</td>\n      <td>1045.164241</td>\n      <td>797.949504</td>\n      <td>947.162176</td>\n      <td>671.898241</td>\n      <td>2680.857729</td>\n      <td>103.469584</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27754</th>\n      <td>id_fca887f42</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-0.4157</td>\n      <td>-0.0461</td>\n      <td>-0.9751</td>\n      <td>0.7702</td>\n      <td>-0.1861</td>\n      <td>0.3608</td>\n      <td>...</td>\n      <td>18.974736</td>\n      <td>70.711281</td>\n      <td>1.157776</td>\n      <td>24.830289</td>\n      <td>10.784656</td>\n      <td>35.640900</td>\n      <td>6.702921</td>\n      <td>49.942489</td>\n      <td>0.300304</td>\n      <td>60.497284</td>\n    </tr>\n    <tr>\n      <th>27759</th>\n      <td>id_fce497048</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.7107</td>\n      <td>-0.3274</td>\n      <td>-0.0099</td>\n      <td>-1.4950</td>\n      <td>0.6673</td>\n      <td>-1.5380</td>\n      <td>...</td>\n      <td>199.430884</td>\n      <td>72.539289</td>\n      <td>1238.336100</td>\n      <td>106.357969</td>\n      <td>3121.009956</td>\n      <td>194.435136</td>\n      <td>651.168324</td>\n      <td>170.485249</td>\n      <td>759.443364</td>\n      <td>248.755984</td>\n    </tr>\n    <tr>\n      <th>27768</th>\n      <td>id_fd75349b2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-0.0776</td>\n      <td>0.4437</td>\n      <td>-0.1530</td>\n      <td>1.2300</td>\n      <td>-0.5804</td>\n      <td>0.0633</td>\n      <td>...</td>\n      <td>0.722500</td>\n      <td>0.222784</td>\n      <td>20.007729</td>\n      <td>47.444544</td>\n      <td>46.090521</td>\n      <td>100.220121</td>\n      <td>3.101121</td>\n      <td>0.405769</td>\n      <td>616.429584</td>\n      <td>43.086096</td>\n    </tr>\n    <tr>\n      <th>27784</th>\n      <td>id_fed0f2fe0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0740</td>\n      <td>0.7350</td>\n      <td>0.3304</td>\n      <td>-0.6764</td>\n      <td>0.1435</td>\n      <td>-1.6810</td>\n      <td>...</td>\n      <td>135.117376</td>\n      <td>64.192144</td>\n      <td>6.990736</td>\n      <td>124.478649</td>\n      <td>76.160529</td>\n      <td>290.838916</td>\n      <td>272.514064</td>\n      <td>151.659225</td>\n      <td>177.102864</td>\n      <td>207.994084</td>\n    </tr>\n    <tr>\n      <th>27794</th>\n      <td>id_ffbb869f2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0960</td>\n      <td>-1.7750</td>\n      <td>-0.3977</td>\n      <td>1.0160</td>\n      <td>-1.3350</td>\n      <td>-0.2207</td>\n      <td>...</td>\n      <td>1.530169</td>\n      <td>43.626025</td>\n      <td>72.454144</td>\n      <td>28.793956</td>\n      <td>0.157609</td>\n      <td>0.937024</td>\n      <td>16.777216</td>\n      <td>4.439449</td>\n      <td>20.903184</td>\n      <td>6.533136</td>\n    </tr>\n  </tbody>\n</table>\n<p>27796 rows × 2620 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = data_df.replace([np.inf, -np.inf], np.nan)\ndata_df = data_df.dropna(how='any', axis=1)\ndata = data_df.copy()","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g_list = [col for col in data.columns[4:] if col.startswith(\"g-\")]\nc_list = [col for col in data.columns[4:] if col.startswith(\"c-\")]\nd_g_list = [col for col in data.columns[4:] if col.startswith(\"d_g-\")]\nd_c_list = [col for col in data.columns[4:] if col.startswith(\"d_c-\")]\ndf_g_list = [col for col in data.columns[4:] if col.startswith(\"df_g-\")]\ndf_c_list = [col for col in data.columns[4:] if col.startswith(\"df_c-\")]\ng_all_list = g_list + d_g_list + df_g_list\nc_all_list = c_list + d_c_list + df_c_list","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, QuantileTransformer\n    \n# Z-score\n#scaler = StandardScaler(with_mean=True, with_std=True)\n\n# RankGauss\nscaler = QuantileTransformer(output_distribution='normal', random_state=SEED)\nsize = len(data[col].values)\n\n# Without Z-scored gene expression data\nfor col in tqdm(data.columns[4+len(g_list):]):\n    \n    raw = data[col].values.reshape(size, 1)\n    scaler.fit(raw)\n\n    data[col] = scaler.transform(raw).reshape(1, size)[0]\n    \ndata","execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1844.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a69f57f6bfd49108f993635e2e71faa"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"             sig_id  cp_type  cp_time  cp_dose     g-0     g-1     g-2  \\\n0      id_000644bb2        1        0        0  1.0620  0.5577 -0.2479   \n5      id_001762a82        1        0        0 -0.6111  0.2941 -0.9901   \n36     id_00762e877        1        0        0 -0.4026  0.1662 -0.6307   \n38     id_0079af0fb        0        0        0 -0.1636 -1.8230 -0.5211   \n39     id_0079d45d3        1        0        0  1.6310 -2.1430 -0.0943   \n...             ...      ...      ...      ...     ...     ...     ...   \n27754  id_fca887f42        1        1        1 -0.4157 -0.0461 -0.9751   \n27759  id_fce497048        1        1        1  0.7107 -0.3274 -0.0099   \n27768  id_fd75349b2        1        1        1 -0.0776  0.4437 -0.1530   \n27784  id_fed0f2fe0        1        1        1 -1.0740  0.7350  0.3304   \n27794  id_ffbb869f2        1        1        1 -1.0960 -1.7750 -0.3977   \n\n          g-3     g-4     g-5  ...   df_c-90   df_c-91   df_c-92   df_c-93  \\\n0     -0.6208 -0.1944 -1.0120  ... -1.810818 -1.556684  0.248606 -1.083179   \n5      0.2277  1.2810  0.5203  ...  1.204512  0.578443  1.006571  1.139912   \n36    -0.4438 -0.5992 -0.2523  ...  0.722254 -0.057595 -0.552164 -0.559241   \n38     0.3054 -1.1280  0.6041  ...  0.374101  0.776318  0.609079  0.186897   \n39    -1.1990  0.4869 -0.0935  ...  1.545960  1.713131  1.361963  1.543328   \n...       ...     ...     ...  ...       ...       ...       ...       ...   \n27754  0.7702 -0.1861  0.3608  ... -0.147978  0.572841 -1.187832 -0.046089   \n27759 -1.4950  0.6673 -1.5380  ...  1.145838  0.589930  1.501425  0.751316   \n27768  1.2300 -0.5804  0.0633  ... -1.310017 -1.616881 -0.099856  0.290319   \n27784 -0.6764  0.1435 -1.6810  ...  0.949547  0.513294 -0.572693  0.836947   \n27794  1.0160 -1.3350 -0.2207  ... -1.093272  0.292672  0.617046  0.026101   \n\n        df_c-94   df_c-95   df_c-96   df_c-97   df_c-98   df_c-99  \n0      0.184056 -0.422826  0.452858 -1.087786 -1.717424 -1.652775  \n5      0.620313 -1.168574  0.544603 -1.732771 -0.122754  0.767287  \n36    -0.181398 -0.197771 -0.918947  0.003369 -1.947569 -1.204362  \n38    -1.233969 -0.059989 -0.958117 -0.570161 -0.453190  1.387738  \n39     1.492319  1.588429  1.517699  1.513423  1.721750  0.753663  \n...         ...       ...       ...       ...       ...       ...  \n27754 -0.472173  0.185676 -0.600581  0.372026 -1.536801  0.434025  \n27759  1.638799  1.135045  1.459280  1.082334  1.356379  1.247627  \n27768  0.222431  0.772244 -0.884043 -1.473699  1.308167  0.237404  \n27784  0.502350  1.305647  1.236926  1.021927  0.945616  1.153026  \n27794 -1.747524 -1.242866 -0.207664 -0.742805 -0.091712 -0.628309  \n\n[27796 rows x 2620 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>g-0</th>\n      <th>g-1</th>\n      <th>g-2</th>\n      <th>g-3</th>\n      <th>g-4</th>\n      <th>g-5</th>\n      <th>...</th>\n      <th>df_c-90</th>\n      <th>df_c-91</th>\n      <th>df_c-92</th>\n      <th>df_c-93</th>\n      <th>df_c-94</th>\n      <th>df_c-95</th>\n      <th>df_c-96</th>\n      <th>df_c-97</th>\n      <th>df_c-98</th>\n      <th>df_c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0620</td>\n      <td>0.5577</td>\n      <td>-0.2479</td>\n      <td>-0.6208</td>\n      <td>-0.1944</td>\n      <td>-1.0120</td>\n      <td>...</td>\n      <td>-1.810818</td>\n      <td>-1.556684</td>\n      <td>0.248606</td>\n      <td>-1.083179</td>\n      <td>0.184056</td>\n      <td>-0.422826</td>\n      <td>0.452858</td>\n      <td>-1.087786</td>\n      <td>-1.717424</td>\n      <td>-1.652775</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>id_001762a82</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.6111</td>\n      <td>0.2941</td>\n      <td>-0.9901</td>\n      <td>0.2277</td>\n      <td>1.2810</td>\n      <td>0.5203</td>\n      <td>...</td>\n      <td>1.204512</td>\n      <td>0.578443</td>\n      <td>1.006571</td>\n      <td>1.139912</td>\n      <td>0.620313</td>\n      <td>-1.168574</td>\n      <td>0.544603</td>\n      <td>-1.732771</td>\n      <td>-0.122754</td>\n      <td>0.767287</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>id_00762e877</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.4026</td>\n      <td>0.1662</td>\n      <td>-0.6307</td>\n      <td>-0.4438</td>\n      <td>-0.5992</td>\n      <td>-0.2523</td>\n      <td>...</td>\n      <td>0.722254</td>\n      <td>-0.057595</td>\n      <td>-0.552164</td>\n      <td>-0.559241</td>\n      <td>-0.181398</td>\n      <td>-0.197771</td>\n      <td>-0.918947</td>\n      <td>0.003369</td>\n      <td>-1.947569</td>\n      <td>-1.204362</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>id_0079af0fb</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-0.1636</td>\n      <td>-1.8230</td>\n      <td>-0.5211</td>\n      <td>0.3054</td>\n      <td>-1.1280</td>\n      <td>0.6041</td>\n      <td>...</td>\n      <td>0.374101</td>\n      <td>0.776318</td>\n      <td>0.609079</td>\n      <td>0.186897</td>\n      <td>-1.233969</td>\n      <td>-0.059989</td>\n      <td>-0.958117</td>\n      <td>-0.570161</td>\n      <td>-0.453190</td>\n      <td>1.387738</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>id_0079d45d3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.6310</td>\n      <td>-2.1430</td>\n      <td>-0.0943</td>\n      <td>-1.1990</td>\n      <td>0.4869</td>\n      <td>-0.0935</td>\n      <td>...</td>\n      <td>1.545960</td>\n      <td>1.713131</td>\n      <td>1.361963</td>\n      <td>1.543328</td>\n      <td>1.492319</td>\n      <td>1.588429</td>\n      <td>1.517699</td>\n      <td>1.513423</td>\n      <td>1.721750</td>\n      <td>0.753663</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27754</th>\n      <td>id_fca887f42</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-0.4157</td>\n      <td>-0.0461</td>\n      <td>-0.9751</td>\n      <td>0.7702</td>\n      <td>-0.1861</td>\n      <td>0.3608</td>\n      <td>...</td>\n      <td>-0.147978</td>\n      <td>0.572841</td>\n      <td>-1.187832</td>\n      <td>-0.046089</td>\n      <td>-0.472173</td>\n      <td>0.185676</td>\n      <td>-0.600581</td>\n      <td>0.372026</td>\n      <td>-1.536801</td>\n      <td>0.434025</td>\n    </tr>\n    <tr>\n      <th>27759</th>\n      <td>id_fce497048</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.7107</td>\n      <td>-0.3274</td>\n      <td>-0.0099</td>\n      <td>-1.4950</td>\n      <td>0.6673</td>\n      <td>-1.5380</td>\n      <td>...</td>\n      <td>1.145838</td>\n      <td>0.589930</td>\n      <td>1.501425</td>\n      <td>0.751316</td>\n      <td>1.638799</td>\n      <td>1.135045</td>\n      <td>1.459280</td>\n      <td>1.082334</td>\n      <td>1.356379</td>\n      <td>1.247627</td>\n    </tr>\n    <tr>\n      <th>27768</th>\n      <td>id_fd75349b2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-0.0776</td>\n      <td>0.4437</td>\n      <td>-0.1530</td>\n      <td>1.2300</td>\n      <td>-0.5804</td>\n      <td>0.0633</td>\n      <td>...</td>\n      <td>-1.310017</td>\n      <td>-1.616881</td>\n      <td>-0.099856</td>\n      <td>0.290319</td>\n      <td>0.222431</td>\n      <td>0.772244</td>\n      <td>-0.884043</td>\n      <td>-1.473699</td>\n      <td>1.308167</td>\n      <td>0.237404</td>\n    </tr>\n    <tr>\n      <th>27784</th>\n      <td>id_fed0f2fe0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0740</td>\n      <td>0.7350</td>\n      <td>0.3304</td>\n      <td>-0.6764</td>\n      <td>0.1435</td>\n      <td>-1.6810</td>\n      <td>...</td>\n      <td>0.949547</td>\n      <td>0.513294</td>\n      <td>-0.572693</td>\n      <td>0.836947</td>\n      <td>0.502350</td>\n      <td>1.305647</td>\n      <td>1.236926</td>\n      <td>1.021927</td>\n      <td>0.945616</td>\n      <td>1.153026</td>\n    </tr>\n    <tr>\n      <th>27794</th>\n      <td>id_ffbb869f2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>-1.0960</td>\n      <td>-1.7750</td>\n      <td>-0.3977</td>\n      <td>1.0160</td>\n      <td>-1.3350</td>\n      <td>-0.2207</td>\n      <td>...</td>\n      <td>-1.093272</td>\n      <td>0.292672</td>\n      <td>0.617046</td>\n      <td>0.026101</td>\n      <td>-1.747524</td>\n      <td>-1.242866</td>\n      <td>-0.207664</td>\n      <td>-0.742805</td>\n      <td>-0.091712</td>\n      <td>-0.628309</td>\n    </tr>\n  </tbody>\n</table>\n<p>27796 rows × 2620 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_df = data.iloc[:, 4:].copy()","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.cp_type = data_df.cp_type.astype('int16')\ndata_df.cp_time = data_df.cp_time.astype('int16')\ndata_df.cp_dose = data_df.cp_dose.astype('int16')","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nn_clusters = 7\n\ndef create_cluster(data, features, kind, n_clusters):\n    \n    data_ = data[features].copy()\n    kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data_)\n    data[f'clusters_{kind}'] = kmeans.labels_[:data.shape[0]]\n \n    return data\n\n\ndef detect_cluster(data, feature_list, kind_list, n_clusters):\n    \n    for idx, feature in enumerate(tqdm(feature_list)):\n        data = create_cluster(data, feature, kind=kind_list[idx], n_clusters=n_clusters)\n    \n    clusters = data.iloc[:, -len(feature_list):].copy()\n    \n    return clusters","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_list = (g_list, c_list, d_g_list, d_c_list, df_g_list, df_c_list, g_all_list, c_all_list)\nkind_list = ('g', 'c', 'd_g', 'd_c', 'df_g', 'df_c', 'g_all', 'c_all')\n\n\nclusters = detect_cluster(data, feature_list, kind_list, n_clusters)\nclusters","execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee9445877a747609352942d961be4fd"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"       clusters_g  clusters_c  clusters_d_g  clusters_d_c  clusters_df_g  \\\n0               6           2             5             4              1   \n5               0           3             3             0              6   \n36              0           2             5             4              6   \n38              3           0             2             6              1   \n39              5           5             1             1              2   \n...           ...         ...           ...           ...            ...   \n27754           6           0             4             6              1   \n27759           4           5             1             1              0   \n27768           6           0             4             4              3   \n27784           3           4             5             1              3   \n27794           3           2             2             4              3   \n\n       clusters_df_c  clusters_g_all  clusters_c_all  \n0                  2               0               0  \n5                  1               4               5  \n36                 2               0               0  \n38                 4               4               2  \n39                 0               6               1  \n...              ...             ...             ...  \n27754              4               0               2  \n27759              0               2               1  \n27768              4               0               2  \n27784              1               3               3  \n27794              2               3               0  \n\n[27796 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clusters_g</th>\n      <th>clusters_c</th>\n      <th>clusters_d_g</th>\n      <th>clusters_d_c</th>\n      <th>clusters_df_g</th>\n      <th>clusters_df_c</th>\n      <th>clusters_g_all</th>\n      <th>clusters_c_all</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27754</th>\n      <td>6</td>\n      <td>0</td>\n      <td>4</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>27759</th>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27768</th>\n      <td>6</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>27784</th>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>27794</th>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>27796 rows × 8 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count cluster types\nfor i in tqdm(range(n_clusters-1, -1, -1)):\n    clusters[f\"cnt_{i}\"] = clusters.apply(lambda x: (x == i).sum(), axis=1)\nclusters","execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef3c87d94f645608e770eb6b666739f"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"       clusters_g  clusters_c  clusters_d_g  clusters_d_c  clusters_df_g  \\\n0               6           2             5             4              1   \n5               0           3             3             0              6   \n36              0           2             5             4              6   \n38              3           0             2             6              1   \n39              5           5             1             1              2   \n...           ...         ...           ...           ...            ...   \n27754           6           0             4             6              1   \n27759           4           5             1             1              0   \n27768           6           0             4             4              3   \n27784           3           4             5             1              3   \n27794           3           2             2             4              3   \n\n       clusters_df_c  clusters_g_all  clusters_c_all  cnt_6  cnt_5  cnt_4  \\\n0                  2               0               0      1      1      1   \n5                  1               4               5      1      1      1   \n36                 2               0               0      1      1      1   \n38                 4               4               2      1      0      2   \n39                 0               6               1      1      2      0   \n...              ...             ...             ...    ...    ...    ...   \n27754              4               0               2      2      0      2   \n27759              0               2               1      0      1      1   \n27768              4               0               2      1      0      3   \n27784              1               3               3      0      1      1   \n27794              2               3               0      0      0      1   \n\n       cnt_3  cnt_2  cnt_1  cnt_0  \n0          0      2      4      3  \n5          2      1      5      2  \n36         0      2      3      4  \n38         1      3      3      2  \n39         0      2      4      3  \n...      ...    ...    ...    ...  \n27754      0      3      1      4  \n27759      0      1      6      4  \n27768      2      2      1      3  \n27784      4      0      4      2  \n27794      3      3      1      3  \n\n[27796 rows x 15 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>clusters_g</th>\n      <th>clusters_c</th>\n      <th>clusters_d_g</th>\n      <th>clusters_d_c</th>\n      <th>clusters_df_g</th>\n      <th>clusters_df_c</th>\n      <th>clusters_g_all</th>\n      <th>clusters_c_all</th>\n      <th>cnt_6</th>\n      <th>cnt_5</th>\n      <th>cnt_4</th>\n      <th>cnt_3</th>\n      <th>cnt_2</th>\n      <th>cnt_1</th>\n      <th>cnt_0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>6</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27754</th>\n      <td>6</td>\n      <td>0</td>\n      <td>4</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>27759</th>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>27768</th>\n      <td>6</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>27784</th>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>27794</th>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>27796 rows × 15 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fe_stats(df, features, kind):\n    df_ = df.copy()\n    MAX, MIN = df_[features].max(axis = 1), df_[features].min(axis = 1)\n    Kurt = df_[features].kurtosis(axis = 1)\n    Skew = df_[features].skew(axis = 1)\n    \n    df_[f'{kind}_max'] = MAX\n    df_[f'{kind}_min'] = MIN\n    df_[f'{kind}_max_min'] = (MAX * MIN)**2\n    \n    df_[f'{kind}_kurt'] = Kurt**3\n    df_[f'{kind}_skew'] = Skew**3\n    df_[f'{kind}_max_kurt'] = MAX * Kurt\n    df_[f'{kind}_max_skew'] = MAX * Skew\n    df_[f'{kind}_kurt_skew'] = Kurt * Skew\n    \n    df_[f'{kind}_sum'] = (df_[features].sum(axis = 1))**3\n    df_[f'{kind}_mean'] = (df_[features].mean(axis = 1))**3\n    df_[f'{kind}_median'] = (df_[features].median(axis = 1))**3\n    df_[f'{kind}_mad'] = (df_[features].mad(axis = 1))**3\n    df_[f'{kind}_std'] = (df_[features].std(axis = 1))**3\n\n    return df_\n\ndef detect_stats(data, feature_list, kind_list):\n    \n    for idx, feature in enumerate(tqdm(feature_list)):\n        data = fe_stats(data, feature, kind=kind_list[idx])\n\n    stats = data.iloc[:, -9*len(feature_list):].copy()\n    \n    return stats","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats = detect_stats(data, feature_list, kind_list)\nstats","execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1b86f84163345e3a947b5f885ed7747"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"       d_g_max_skew  d_g_kurt_skew       d_g_sum      d_g_mean  d_g_median  \\\n0          1.773997       1.122617 -1.278176e+03 -2.778042e-06   -0.000164   \n5         -0.810013      -0.668520  4.686249e+04  1.018529e-04    0.000493   \n36         0.774587       0.831640  2.368342e+02  5.147454e-07   -0.000019   \n38         0.607893       0.323852  1.909071e+03  4.149255e-06    0.000007   \n39        -0.867726      -0.279692  1.404200e+04  3.051948e-05    0.001977   \n...             ...            ...           ...           ...         ...   \n27754      1.124029       0.487651 -4.176693e+03 -9.077801e-06   -0.000059   \n27759     -0.603212      -0.187947 -3.668553e+05 -7.973389e-04   -0.001380   \n27768      2.355228       1.710186  1.291874e+02  2.807813e-07   -0.000040   \n27784     -0.465225      -0.172956 -6.489317e+04 -1.410416e-04   -0.000112   \n27794      0.452458       0.229407  2.381521e+06  5.176098e-03    0.027547   \n\n        d_g_mad   d_g_std   d_c_max   d_c_min  d_c_max_min  ...   c_all_kurt  \\\n0      0.421547  0.921185  1.479033 -0.843085     1.554884  ...   702.924851   \n5      0.450344  1.019499  1.859501 -0.862363     2.571423  ...   119.845739   \n36     0.460320  1.141932  0.980110 -1.090500     1.142353  ...  1105.732838   \n38     0.553510  1.126814  1.185165 -1.179324     1.953547  ...     0.012519   \n39     2.540625  5.120534 -0.393566 -1.898526     0.558301  ...    -3.218037   \n...         ...       ...       ...       ...          ...  ...          ...   \n27754  0.459287  0.896787  1.800581 -1.739289     9.807738  ...    29.828422   \n27759  1.934329  3.692947  0.980837 -2.611712     6.562119  ...    -2.327266   \n27768  0.862901  1.992174  2.611712 -1.468736    14.714241  ...   270.636976   \n27784  0.540292  1.000059  1.177806 -2.101746     6.127853  ...     3.647902   \n27794  0.681515  1.388882  1.717725 -1.621529     7.758125  ...    -0.000630   \n\n       c_all_skew  c_all_max_kurt  c_all_max_skew  c_all_kurt_skew  \\\n0       -6.600307       19.580500       -4.130876       -16.678526   \n5        1.274329       25.634344        5.636921         5.345251   \n36      -9.744666       16.710012       -3.451562       -22.087118   \n38      -0.001557        0.421912       -0.210613        -0.026914   \n39       0.276399       -2.778375        1.225855        -0.961706   \n...           ...             ...             ...              ...   \n27754   -0.219709        6.451519       -1.255263        -1.871369   \n27759    0.186799       -2.454153        1.058631        -0.757543   \n27768   -3.786773       16.893553       -4.070818       -10.082126   \n27784    0.000002        2.470916        0.021522         0.020641   \n27794   -0.002918       -0.147276       -0.245468         0.012252   \n\n          c_all_sum    c_all_mean  c_all_median  c_all_mad  c_all_std  \n0      1.200612e+04  4.446710e-04  6.841772e-03   0.288784   0.812540  \n5      2.433312e+07  9.012267e-01  7.215851e-01   0.573522   2.324385  \n36    -6.350024e+02 -2.351861e-05  8.186017e-05   0.246688   0.682099  \n38    -3.831963e+04 -1.419245e-03 -9.070147e-04   0.130688   0.284034  \n39    -1.693883e+06 -6.273639e-02 -1.606729e+00   2.088183   2.623831  \n...             ...           ...           ...        ...        ...  \n27754 -5.882616e+04 -2.178747e-03 -5.373233e-04   0.333428   0.633158  \n27759 -2.156482e+06 -7.986971e-02 -1.098727e+00   1.533818   2.067397  \n27768 -5.987463e+03 -2.217579e-04 -1.400804e-07   0.521840   1.280537  \n27784 -9.736824e+05 -3.606231e-02 -2.744378e-01   0.443372   0.740543  \n27794  6.709705e+00  2.485076e-07 -2.467269e-07   0.194441   0.389256  \n\n[27796 rows x 72 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>d_g_max_skew</th>\n      <th>d_g_kurt_skew</th>\n      <th>d_g_sum</th>\n      <th>d_g_mean</th>\n      <th>d_g_median</th>\n      <th>d_g_mad</th>\n      <th>d_g_std</th>\n      <th>d_c_max</th>\n      <th>d_c_min</th>\n      <th>d_c_max_min</th>\n      <th>...</th>\n      <th>c_all_kurt</th>\n      <th>c_all_skew</th>\n      <th>c_all_max_kurt</th>\n      <th>c_all_max_skew</th>\n      <th>c_all_kurt_skew</th>\n      <th>c_all_sum</th>\n      <th>c_all_mean</th>\n      <th>c_all_median</th>\n      <th>c_all_mad</th>\n      <th>c_all_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.773997</td>\n      <td>1.122617</td>\n      <td>-1.278176e+03</td>\n      <td>-2.778042e-06</td>\n      <td>-0.000164</td>\n      <td>0.421547</td>\n      <td>0.921185</td>\n      <td>1.479033</td>\n      <td>-0.843085</td>\n      <td>1.554884</td>\n      <td>...</td>\n      <td>702.924851</td>\n      <td>-6.600307</td>\n      <td>19.580500</td>\n      <td>-4.130876</td>\n      <td>-16.678526</td>\n      <td>1.200612e+04</td>\n      <td>4.446710e-04</td>\n      <td>6.841772e-03</td>\n      <td>0.288784</td>\n      <td>0.812540</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.810013</td>\n      <td>-0.668520</td>\n      <td>4.686249e+04</td>\n      <td>1.018529e-04</td>\n      <td>0.000493</td>\n      <td>0.450344</td>\n      <td>1.019499</td>\n      <td>1.859501</td>\n      <td>-0.862363</td>\n      <td>2.571423</td>\n      <td>...</td>\n      <td>119.845739</td>\n      <td>1.274329</td>\n      <td>25.634344</td>\n      <td>5.636921</td>\n      <td>5.345251</td>\n      <td>2.433312e+07</td>\n      <td>9.012267e-01</td>\n      <td>7.215851e-01</td>\n      <td>0.573522</td>\n      <td>2.324385</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.774587</td>\n      <td>0.831640</td>\n      <td>2.368342e+02</td>\n      <td>5.147454e-07</td>\n      <td>-0.000019</td>\n      <td>0.460320</td>\n      <td>1.141932</td>\n      <td>0.980110</td>\n      <td>-1.090500</td>\n      <td>1.142353</td>\n      <td>...</td>\n      <td>1105.732838</td>\n      <td>-9.744666</td>\n      <td>16.710012</td>\n      <td>-3.451562</td>\n      <td>-22.087118</td>\n      <td>-6.350024e+02</td>\n      <td>-2.351861e-05</td>\n      <td>8.186017e-05</td>\n      <td>0.246688</td>\n      <td>0.682099</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.607893</td>\n      <td>0.323852</td>\n      <td>1.909071e+03</td>\n      <td>4.149255e-06</td>\n      <td>0.000007</td>\n      <td>0.553510</td>\n      <td>1.126814</td>\n      <td>1.185165</td>\n      <td>-1.179324</td>\n      <td>1.953547</td>\n      <td>...</td>\n      <td>0.012519</td>\n      <td>-0.001557</td>\n      <td>0.421912</td>\n      <td>-0.210613</td>\n      <td>-0.026914</td>\n      <td>-3.831963e+04</td>\n      <td>-1.419245e-03</td>\n      <td>-9.070147e-04</td>\n      <td>0.130688</td>\n      <td>0.284034</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>-0.867726</td>\n      <td>-0.279692</td>\n      <td>1.404200e+04</td>\n      <td>3.051948e-05</td>\n      <td>0.001977</td>\n      <td>2.540625</td>\n      <td>5.120534</td>\n      <td>-0.393566</td>\n      <td>-1.898526</td>\n      <td>0.558301</td>\n      <td>...</td>\n      <td>-3.218037</td>\n      <td>0.276399</td>\n      <td>-2.778375</td>\n      <td>1.225855</td>\n      <td>-0.961706</td>\n      <td>-1.693883e+06</td>\n      <td>-6.273639e-02</td>\n      <td>-1.606729e+00</td>\n      <td>2.088183</td>\n      <td>2.623831</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27754</th>\n      <td>1.124029</td>\n      <td>0.487651</td>\n      <td>-4.176693e+03</td>\n      <td>-9.077801e-06</td>\n      <td>-0.000059</td>\n      <td>0.459287</td>\n      <td>0.896787</td>\n      <td>1.800581</td>\n      <td>-1.739289</td>\n      <td>9.807738</td>\n      <td>...</td>\n      <td>29.828422</td>\n      <td>-0.219709</td>\n      <td>6.451519</td>\n      <td>-1.255263</td>\n      <td>-1.871369</td>\n      <td>-5.882616e+04</td>\n      <td>-2.178747e-03</td>\n      <td>-5.373233e-04</td>\n      <td>0.333428</td>\n      <td>0.633158</td>\n    </tr>\n    <tr>\n      <th>27759</th>\n      <td>-0.603212</td>\n      <td>-0.187947</td>\n      <td>-3.668553e+05</td>\n      <td>-7.973389e-04</td>\n      <td>-0.001380</td>\n      <td>1.934329</td>\n      <td>3.692947</td>\n      <td>0.980837</td>\n      <td>-2.611712</td>\n      <td>6.562119</td>\n      <td>...</td>\n      <td>-2.327266</td>\n      <td>0.186799</td>\n      <td>-2.454153</td>\n      <td>1.058631</td>\n      <td>-0.757543</td>\n      <td>-2.156482e+06</td>\n      <td>-7.986971e-02</td>\n      <td>-1.098727e+00</td>\n      <td>1.533818</td>\n      <td>2.067397</td>\n    </tr>\n    <tr>\n      <th>27768</th>\n      <td>2.355228</td>\n      <td>1.710186</td>\n      <td>1.291874e+02</td>\n      <td>2.807813e-07</td>\n      <td>-0.000040</td>\n      <td>0.862901</td>\n      <td>1.992174</td>\n      <td>2.611712</td>\n      <td>-1.468736</td>\n      <td>14.714241</td>\n      <td>...</td>\n      <td>270.636976</td>\n      <td>-3.786773</td>\n      <td>16.893553</td>\n      <td>-4.070818</td>\n      <td>-10.082126</td>\n      <td>-5.987463e+03</td>\n      <td>-2.217579e-04</td>\n      <td>-1.400804e-07</td>\n      <td>0.521840</td>\n      <td>1.280537</td>\n    </tr>\n    <tr>\n      <th>27784</th>\n      <td>-0.465225</td>\n      <td>-0.172956</td>\n      <td>-6.489317e+04</td>\n      <td>-1.410416e-04</td>\n      <td>-0.000112</td>\n      <td>0.540292</td>\n      <td>1.000059</td>\n      <td>1.177806</td>\n      <td>-2.101746</td>\n      <td>6.127853</td>\n      <td>...</td>\n      <td>3.647902</td>\n      <td>0.000002</td>\n      <td>2.470916</td>\n      <td>0.021522</td>\n      <td>0.020641</td>\n      <td>-9.736824e+05</td>\n      <td>-3.606231e-02</td>\n      <td>-2.744378e-01</td>\n      <td>0.443372</td>\n      <td>0.740543</td>\n    </tr>\n    <tr>\n      <th>27794</th>\n      <td>0.452458</td>\n      <td>0.229407</td>\n      <td>2.381521e+06</td>\n      <td>5.176098e-03</td>\n      <td>0.027547</td>\n      <td>0.681515</td>\n      <td>1.388882</td>\n      <td>1.717725</td>\n      <td>-1.621529</td>\n      <td>7.758125</td>\n      <td>...</td>\n      <td>-0.000630</td>\n      <td>-0.002918</td>\n      <td>-0.147276</td>\n      <td>-0.245468</td>\n      <td>0.012252</td>\n      <td>6.709705e+00</td>\n      <td>2.485076e-07</td>\n      <td>-2.467269e-07</td>\n      <td>0.194441</td>\n      <td>0.389256</td>\n    </tr>\n  </tbody>\n</table>\n<p>27796 rows × 72 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add data with sig_id, cp_type, cp_time, and cp_dose\ndata = pd.concat([data.iloc[:, :4], clusters], axis=1)\ndata = pd.concat([data, stats], axis=1)\ndata = pd.concat([data, std_df], axis=1)\ndata","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"             sig_id  cp_type  cp_time  cp_dose  clusters_g  clusters_c  \\\n0      id_000644bb2        1        0        0           6           2   \n5      id_001762a82        1        0        0           0           3   \n36     id_00762e877        1        0        0           0           2   \n38     id_0079af0fb        0        0        0           3           0   \n39     id_0079d45d3        1        0        0           5           5   \n...             ...      ...      ...      ...         ...         ...   \n27754  id_fca887f42        1        1        1           6           0   \n27759  id_fce497048        1        1        1           4           5   \n27768  id_fd75349b2        1        1        1           6           0   \n27784  id_fed0f2fe0        1        1        1           3           4   \n27794  id_ffbb869f2        1        1        1           3           2   \n\n       clusters_d_g  clusters_d_c  clusters_df_g  clusters_df_c  ...  \\\n0                 5             4              1              2  ...   \n5                 3             0              6              1  ...   \n36                5             4              6              2  ...   \n38                2             6              1              4  ...   \n39                1             1              2              0  ...   \n...             ...           ...            ...            ...  ...   \n27754             4             6              1              4  ...   \n27759             1             1              0              0  ...   \n27768             4             4              3              4  ...   \n27784             5             1              3              1  ...   \n27794             2             4              3              2  ...   \n\n        df_c-90   df_c-91   df_c-92   df_c-93   df_c-94   df_c-95   df_c-96  \\\n0     -1.810818 -1.556684  0.248606 -1.083179  0.184056 -0.422826  0.452858   \n5      1.204512  0.578443  1.006571  1.139912  0.620313 -1.168574  0.544603   \n36     0.722254 -0.057595 -0.552164 -0.559241 -0.181398 -0.197771 -0.918947   \n38     0.374101  0.776318  0.609079  0.186897 -1.233969 -0.059989 -0.958117   \n39     1.545960  1.713131  1.361963  1.543328  1.492319  1.588429  1.517699   \n...         ...       ...       ...       ...       ...       ...       ...   \n27754 -0.147978  0.572841 -1.187832 -0.046089 -0.472173  0.185676 -0.600581   \n27759  1.145838  0.589930  1.501425  0.751316  1.638799  1.135045  1.459280   \n27768 -1.310017 -1.616881 -0.099856  0.290319  0.222431  0.772244 -0.884043   \n27784  0.949547  0.513294 -0.572693  0.836947  0.502350  1.305647  1.236926   \n27794 -1.093272  0.292672  0.617046  0.026101 -1.747524 -1.242866 -0.207664   \n\n        df_c-97   df_c-98   df_c-99  \n0     -1.087786 -1.717424 -1.652775  \n5     -1.732771 -0.122754  0.767287  \n36     0.003369 -1.947569 -1.204362  \n38    -0.570161 -0.453190  1.387738  \n39     1.513423  1.721750  0.753663  \n...         ...       ...       ...  \n27754  0.372026 -1.536801  0.434025  \n27759  1.082334  1.356379  1.247627  \n27768 -1.473699  1.308167  0.237404  \n27784  1.021927  0.945616  1.153026  \n27794 -0.742805 -0.091712 -0.628309  \n\n[27796 rows x 2707 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>clusters_g</th>\n      <th>clusters_c</th>\n      <th>clusters_d_g</th>\n      <th>clusters_d_c</th>\n      <th>clusters_df_g</th>\n      <th>clusters_df_c</th>\n      <th>...</th>\n      <th>df_c-90</th>\n      <th>df_c-91</th>\n      <th>df_c-92</th>\n      <th>df_c-93</th>\n      <th>df_c-94</th>\n      <th>df_c-95</th>\n      <th>df_c-96</th>\n      <th>df_c-97</th>\n      <th>df_c-98</th>\n      <th>df_c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>-1.810818</td>\n      <td>-1.556684</td>\n      <td>0.248606</td>\n      <td>-1.083179</td>\n      <td>0.184056</td>\n      <td>-0.422826</td>\n      <td>0.452858</td>\n      <td>-1.087786</td>\n      <td>-1.717424</td>\n      <td>-1.652775</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>id_001762a82</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1.204512</td>\n      <td>0.578443</td>\n      <td>1.006571</td>\n      <td>1.139912</td>\n      <td>0.620313</td>\n      <td>-1.168574</td>\n      <td>0.544603</td>\n      <td>-1.732771</td>\n      <td>-0.122754</td>\n      <td>0.767287</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>id_00762e877</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>6</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.722254</td>\n      <td>-0.057595</td>\n      <td>-0.552164</td>\n      <td>-0.559241</td>\n      <td>-0.181398</td>\n      <td>-0.197771</td>\n      <td>-0.918947</td>\n      <td>0.003369</td>\n      <td>-1.947569</td>\n      <td>-1.204362</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>id_0079af0fb</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0.374101</td>\n      <td>0.776318</td>\n      <td>0.609079</td>\n      <td>0.186897</td>\n      <td>-1.233969</td>\n      <td>-0.059989</td>\n      <td>-0.958117</td>\n      <td>-0.570161</td>\n      <td>-0.453190</td>\n      <td>1.387738</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>id_0079d45d3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1.545960</td>\n      <td>1.713131</td>\n      <td>1.361963</td>\n      <td>1.543328</td>\n      <td>1.492319</td>\n      <td>1.588429</td>\n      <td>1.517699</td>\n      <td>1.513423</td>\n      <td>1.721750</td>\n      <td>0.753663</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27754</th>\n      <td>id_fca887f42</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>6</td>\n      <td>0</td>\n      <td>4</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>...</td>\n      <td>-0.147978</td>\n      <td>0.572841</td>\n      <td>-1.187832</td>\n      <td>-0.046089</td>\n      <td>-0.472173</td>\n      <td>0.185676</td>\n      <td>-0.600581</td>\n      <td>0.372026</td>\n      <td>-1.536801</td>\n      <td>0.434025</td>\n    </tr>\n    <tr>\n      <th>27759</th>\n      <td>id_fce497048</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1.145838</td>\n      <td>0.589930</td>\n      <td>1.501425</td>\n      <td>0.751316</td>\n      <td>1.638799</td>\n      <td>1.135045</td>\n      <td>1.459280</td>\n      <td>1.082334</td>\n      <td>1.356379</td>\n      <td>1.247627</td>\n    </tr>\n    <tr>\n      <th>27768</th>\n      <td>id_fd75349b2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>6</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>-1.310017</td>\n      <td>-1.616881</td>\n      <td>-0.099856</td>\n      <td>0.290319</td>\n      <td>0.222431</td>\n      <td>0.772244</td>\n      <td>-0.884043</td>\n      <td>-1.473699</td>\n      <td>1.308167</td>\n      <td>0.237404</td>\n    </tr>\n    <tr>\n      <th>27784</th>\n      <td>id_fed0f2fe0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.949547</td>\n      <td>0.513294</td>\n      <td>-0.572693</td>\n      <td>0.836947</td>\n      <td>0.502350</td>\n      <td>1.305647</td>\n      <td>1.236926</td>\n      <td>1.021927</td>\n      <td>0.945616</td>\n      <td>1.153026</td>\n    </tr>\n    <tr>\n      <th>27794</th>\n      <td>id_ffbb869f2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>...</td>\n      <td>-1.093272</td>\n      <td>0.292672</td>\n      <td>0.617046</td>\n      <td>0.026101</td>\n      <td>-1.747524</td>\n      <td>-1.242866</td>\n      <td>-0.207664</td>\n      <td>-0.742805</td>\n      <td>-0.091712</td>\n      <td>-0.628309</td>\n    </tr>\n  </tbody>\n</table>\n<p>27796 rows × 2707 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create feature\nimport itertools\ndef CreateFeat(df):\n  def func_product(row):\n    return (row[col1]) * (row[col2])\n  def func_division(row):\n    delta = 1e-6\n    return (row[col1]+delta) / (row[col2]+delta) \n  \n  Columns = df.columns \n  for col1, col2 in tqdm(tuple(itertools.permutations(Columns, 2))):\n        df[f\"{col1}_{col2}_prd\"] = df[[col1, col2]].apply(func_product, axis=1)\n        df[f\"{col1}_{col2}_div\"] = round(df[[col1, col2]].apply(func_division, axis=1), 0)\n\n  print(f\"Crated {len(df.columns) - len(Columns)} columns\")\n  return df\n\n# Create feature2\ndef CreateFeat2(df):\n  func_list = (\"max\", \"min\", \"mean\", \"median\", \"mad\", \"var\", \"std\")\n  Columns = df.columns \n  for idx, func in enumerate(func_list):\n    print(f\"{idx}/{len(func_list)}: Calucurating... {func}\")\n    for col1, col2 in tqdm(tuple(itertools.permutations(Columns, 2))):\n      df[f\"{col1}_{col2}_{func}\"] = df[[col1, col2]].apply(func, axis=1)\n  print(f\"Crated {len(df.columns) - len(Columns)} columns\")\n  return df\n\n\n#Reduce columens\ndef ReduceCol(df):\n  remove_cols = []\n  Columns = df.columns\n  \n  for col1, col2 in tqdm(tuple(itertools.permutations(Columns, 2))):\n    # constant columns\n    if df[col1].std() == 0: remove_cols.append(col1)\n    \n    # duplicated columns\n    if (col1 not in remove_cols) and (col2 not in remove_cols):\n      x, y = df[col1].values, df[col2].values\n      if np.array_equal(x, y): remove_cols.append(col1)\n\n  df.drop(remove_cols, inplace=True, axis=1)\n  print(f\"Removed {len(remove_cols)} constant & duplicated columns\")\n\n  return df","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create feature based on feature importance with v24 notebook\n\n#important_col = []\n#tmp = CreateFeat(data[important_col])\n#data = pd.concat([data, tmp], axis=1)\n\n\n# Create feature based on feature importance with v24 notebook\n#tmp = CreateFeat2(data[important_col])\n#data = pd.concat([data, tmp], axis=1)\n\n#remove dup colunes\n#data = data.loc[:,~data.columns.duplicated()]\n#tmp = ReduceCol(data.iloc[:,4:])\n#data = pd.concat([data.iloc[:,:4], tmp], axis=1)\n#data","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clipping\nclipping = data.columns[4:]\nfor col in clipping:\n    lower, upper = np.percentile(data[col], [1, 99])\n    data[col] = np.clip(data[col], lower, upper)\ndata","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"             sig_id  cp_type  cp_time  cp_dose  clusters_g  clusters_c  \\\n0      id_000644bb2        1        0        0           6           2   \n5      id_001762a82        1        0        0           0           3   \n36     id_00762e877        1        0        0           0           2   \n38     id_0079af0fb        0        0        0           3           0   \n39     id_0079d45d3        1        0        0           5           5   \n...             ...      ...      ...      ...         ...         ...   \n27754  id_fca887f42        1        1        1           6           0   \n27759  id_fce497048        1        1        1           4           5   \n27768  id_fd75349b2        1        1        1           6           0   \n27784  id_fed0f2fe0        1        1        1           3           4   \n27794  id_ffbb869f2        1        1        1           3           2   \n\n       clusters_d_g  clusters_d_c  clusters_df_g  clusters_df_c  ...  \\\n0                 5             4              1              2  ...   \n5                 3             0              6              1  ...   \n36                5             4              6              2  ...   \n38                2             6              1              4  ...   \n39                1             1              2              0  ...   \n...             ...           ...            ...            ...  ...   \n27754             4             6              1              4  ...   \n27759             1             1              0              0  ...   \n27768             4             4              3              4  ...   \n27784             5             1              3              1  ...   \n27794             2             4              3              2  ...   \n\n        df_c-90   df_c-91   df_c-92   df_c-93   df_c-94   df_c-95   df_c-96  \\\n0     -1.810818 -1.556684  0.248606 -1.083179  0.184056 -0.422826  0.452858   \n5      1.204512  0.578443  1.006571  1.139912  0.620313 -1.168574  0.544603   \n36     0.722254 -0.057595 -0.552164 -0.559241 -0.181398 -0.197771 -0.918947   \n38     0.374101  0.776318  0.609079  0.186897 -1.233969 -0.059989 -0.958117   \n39     1.545960  1.713131  1.361963  1.543328  1.492319  1.588429  1.517699   \n...         ...       ...       ...       ...       ...       ...       ...   \n27754 -0.147978  0.572841 -1.187832 -0.046089 -0.472173  0.185676 -0.600581   \n27759  1.145838  0.589930  1.501425  0.751316  1.638799  1.135045  1.459280   \n27768 -1.310017 -1.616881 -0.099856  0.290319  0.222431  0.772244 -0.884043   \n27784  0.949547  0.513294 -0.572693  0.836947  0.502350  1.305647  1.236926   \n27794 -1.093272  0.292672  0.617046  0.026101 -1.747524 -1.242866 -0.207664   \n\n        df_c-97   df_c-98   df_c-99  \n0     -1.087786 -1.717424 -1.652775  \n5     -1.732771 -0.122754  0.767287  \n36     0.003369 -1.947569 -1.204362  \n38    -0.570161 -0.453190  1.387738  \n39     1.513423  1.721750  0.753663  \n...         ...       ...       ...  \n27754  0.372026 -1.536801  0.434025  \n27759  1.082334  1.356379  1.247627  \n27768 -1.473699  1.308167  0.237404  \n27784  1.021927  0.945616  1.153026  \n27794 -0.742805 -0.091712 -0.628309  \n\n[27796 rows x 2707 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>clusters_g</th>\n      <th>clusters_c</th>\n      <th>clusters_d_g</th>\n      <th>clusters_d_c</th>\n      <th>clusters_df_g</th>\n      <th>clusters_df_c</th>\n      <th>...</th>\n      <th>df_c-90</th>\n      <th>df_c-91</th>\n      <th>df_c-92</th>\n      <th>df_c-93</th>\n      <th>df_c-94</th>\n      <th>df_c-95</th>\n      <th>df_c-96</th>\n      <th>df_c-97</th>\n      <th>df_c-98</th>\n      <th>df_c-99</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>-1.810818</td>\n      <td>-1.556684</td>\n      <td>0.248606</td>\n      <td>-1.083179</td>\n      <td>0.184056</td>\n      <td>-0.422826</td>\n      <td>0.452858</td>\n      <td>-1.087786</td>\n      <td>-1.717424</td>\n      <td>-1.652775</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>id_001762a82</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1.204512</td>\n      <td>0.578443</td>\n      <td>1.006571</td>\n      <td>1.139912</td>\n      <td>0.620313</td>\n      <td>-1.168574</td>\n      <td>0.544603</td>\n      <td>-1.732771</td>\n      <td>-0.122754</td>\n      <td>0.767287</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>id_00762e877</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>6</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.722254</td>\n      <td>-0.057595</td>\n      <td>-0.552164</td>\n      <td>-0.559241</td>\n      <td>-0.181398</td>\n      <td>-0.197771</td>\n      <td>-0.918947</td>\n      <td>0.003369</td>\n      <td>-1.947569</td>\n      <td>-1.204362</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>id_0079af0fb</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0.374101</td>\n      <td>0.776318</td>\n      <td>0.609079</td>\n      <td>0.186897</td>\n      <td>-1.233969</td>\n      <td>-0.059989</td>\n      <td>-0.958117</td>\n      <td>-0.570161</td>\n      <td>-0.453190</td>\n      <td>1.387738</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>id_0079d45d3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1.545960</td>\n      <td>1.713131</td>\n      <td>1.361963</td>\n      <td>1.543328</td>\n      <td>1.492319</td>\n      <td>1.588429</td>\n      <td>1.517699</td>\n      <td>1.513423</td>\n      <td>1.721750</td>\n      <td>0.753663</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27754</th>\n      <td>id_fca887f42</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>6</td>\n      <td>0</td>\n      <td>4</td>\n      <td>6</td>\n      <td>1</td>\n      <td>4</td>\n      <td>...</td>\n      <td>-0.147978</td>\n      <td>0.572841</td>\n      <td>-1.187832</td>\n      <td>-0.046089</td>\n      <td>-0.472173</td>\n      <td>0.185676</td>\n      <td>-0.600581</td>\n      <td>0.372026</td>\n      <td>-1.536801</td>\n      <td>0.434025</td>\n    </tr>\n    <tr>\n      <th>27759</th>\n      <td>id_fce497048</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1.145838</td>\n      <td>0.589930</td>\n      <td>1.501425</td>\n      <td>0.751316</td>\n      <td>1.638799</td>\n      <td>1.135045</td>\n      <td>1.459280</td>\n      <td>1.082334</td>\n      <td>1.356379</td>\n      <td>1.247627</td>\n    </tr>\n    <tr>\n      <th>27768</th>\n      <td>id_fd75349b2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>6</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>-1.310017</td>\n      <td>-1.616881</td>\n      <td>-0.099856</td>\n      <td>0.290319</td>\n      <td>0.222431</td>\n      <td>0.772244</td>\n      <td>-0.884043</td>\n      <td>-1.473699</td>\n      <td>1.308167</td>\n      <td>0.237404</td>\n    </tr>\n    <tr>\n      <th>27784</th>\n      <td>id_fed0f2fe0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.949547</td>\n      <td>0.513294</td>\n      <td>-0.572693</td>\n      <td>0.836947</td>\n      <td>0.502350</td>\n      <td>1.305647</td>\n      <td>1.236926</td>\n      <td>1.021927</td>\n      <td>0.945616</td>\n      <td>1.153026</td>\n    </tr>\n    <tr>\n      <th>27794</th>\n      <td>id_ffbb869f2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>...</td>\n      <td>-1.093272</td>\n      <td>0.292672</td>\n      <td>0.617046</td>\n      <td>0.026101</td>\n      <td>-1.747524</td>\n      <td>-1.242866</td>\n      <td>-0.207664</td>\n      <td>-0.742805</td>\n      <td>-0.091712</td>\n      <td>-0.628309</td>\n    </tr>\n  </tbody>\n</table>\n<p>27796 rows × 2707 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['WHERE'] = Splitdata\ndata = data.sort_index(axis='index')\nSplitdata = data['WHERE'] \ndata ","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"             sig_id  cp_type  cp_time  cp_dose  clusters_g  clusters_c  \\\n0      id_000644bb2        1        0        0           6           2   \n1      id_000779bfc        1        2        0           0           2   \n2      id_000a6266a        1        1        0           0           0   \n3      id_0015fd391        1        1        0           4           4   \n4      id_001626bd3        1        2        1           0           2   \n...             ...      ...      ...      ...         ...         ...   \n27791  id_ff7004b87        1        0        0           3           4   \n27792  id_ff925dd0d        1        0        0           0           6   \n27793  id_ffb710450        1        2        0           6           2   \n27794  id_ffbb869f2        1        1        1           3           2   \n27795  id_ffd5800b6        1        2        0           6           2   \n\n       clusters_d_g  clusters_d_c  clusters_df_g  clusters_df_c  ...  \\\n0                 5             4              1              2  ...   \n1                 3             0              6              2  ...   \n2                 3             6              6              4  ...   \n3                 1             1              0              0  ...   \n4                 3             0              3              2  ...   \n...             ...           ...            ...            ...  ...   \n27791             5             1              6              1  ...   \n27792             3             0              6              2  ...   \n27793             4             0              6              2  ...   \n27794             2             4              3              2  ...   \n27795             5             4              6              2  ...   \n\n        df_c-91   df_c-92   df_c-93   df_c-94   df_c-95   df_c-96   df_c-97  \\\n0     -1.556684  0.248606 -1.083179  0.184056 -0.422826  0.452858 -1.087786   \n1      0.049251 -0.364980 -0.411534 -0.993055 -0.505091 -2.033104 -1.679158   \n2      0.677642 -0.326261 -0.181815  1.284691  0.177076  0.673657  0.134245   \n3      0.697360  1.661223  1.303584  1.045894  1.235752  1.330177  1.019721   \n4     -1.383713  0.367819  0.458141 -0.214427 -0.170683 -1.534088 -0.953044   \n...         ...       ...       ...       ...       ...       ...       ...   \n27791  0.725136  0.251124 -0.237973  0.804368  0.541400  1.233284  1.193864   \n27792 -0.573386  0.499728 -0.284930 -0.160010  0.692818 -1.153627  0.220772   \n27793  0.336446 -0.411697 -1.622411 -0.091706 -0.568255 -1.100391 -0.169549   \n27794  0.292672  0.617046  0.026101 -1.747524 -1.242866 -0.207664 -0.742805   \n27795 -1.542277 -0.071571 -0.236615 -0.937521  0.202393  0.283034 -1.284479   \n\n        df_c-98   df_c-99  WHERE  \n0     -1.717424 -1.652775  train  \n1     -0.856646 -0.482696  train  \n2      1.168034 -0.911960  train  \n3      0.531517  1.067496  train  \n4      0.237818 -0.033880  train  \n...         ...       ...    ...  \n27791  0.415900  0.771102   test  \n27792 -0.327231 -0.125243   test  \n27793  0.531853  0.109870   test  \n27794 -0.091712 -0.628309   test  \n27795  0.111258 -1.282890   test  \n\n[27796 rows x 2708 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_type</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>clusters_g</th>\n      <th>clusters_c</th>\n      <th>clusters_d_g</th>\n      <th>clusters_d_c</th>\n      <th>clusters_df_g</th>\n      <th>clusters_df_c</th>\n      <th>...</th>\n      <th>df_c-91</th>\n      <th>df_c-92</th>\n      <th>df_c-93</th>\n      <th>df_c-94</th>\n      <th>df_c-95</th>\n      <th>df_c-96</th>\n      <th>df_c-97</th>\n      <th>df_c-98</th>\n      <th>df_c-99</th>\n      <th>WHERE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>-1.556684</td>\n      <td>0.248606</td>\n      <td>-1.083179</td>\n      <td>0.184056</td>\n      <td>-0.422826</td>\n      <td>0.452858</td>\n      <td>-1.087786</td>\n      <td>-1.717424</td>\n      <td>-1.652775</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_000779bfc</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.049251</td>\n      <td>-0.364980</td>\n      <td>-0.411534</td>\n      <td>-0.993055</td>\n      <td>-0.505091</td>\n      <td>-2.033104</td>\n      <td>-1.679158</td>\n      <td>-0.856646</td>\n      <td>-0.482696</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_000a6266a</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>6</td>\n      <td>6</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0.677642</td>\n      <td>-0.326261</td>\n      <td>-0.181815</td>\n      <td>1.284691</td>\n      <td>0.177076</td>\n      <td>0.673657</td>\n      <td>0.134245</td>\n      <td>1.168034</td>\n      <td>-0.911960</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_0015fd391</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.697360</td>\n      <td>1.661223</td>\n      <td>1.303584</td>\n      <td>1.045894</td>\n      <td>1.235752</td>\n      <td>1.330177</td>\n      <td>1.019721</td>\n      <td>0.531517</td>\n      <td>1.067496</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_001626bd3</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>...</td>\n      <td>-1.383713</td>\n      <td>0.367819</td>\n      <td>0.458141</td>\n      <td>-0.214427</td>\n      <td>-0.170683</td>\n      <td>-1.534088</td>\n      <td>-0.953044</td>\n      <td>0.237818</td>\n      <td>-0.033880</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27791</th>\n      <td>id_ff7004b87</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>1</td>\n      <td>6</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.725136</td>\n      <td>0.251124</td>\n      <td>-0.237973</td>\n      <td>0.804368</td>\n      <td>0.541400</td>\n      <td>1.233284</td>\n      <td>1.193864</td>\n      <td>0.415900</td>\n      <td>0.771102</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>27792</th>\n      <td>id_ff925dd0d</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>3</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2</td>\n      <td>...</td>\n      <td>-0.573386</td>\n      <td>0.499728</td>\n      <td>-0.284930</td>\n      <td>-0.160010</td>\n      <td>0.692818</td>\n      <td>-1.153627</td>\n      <td>0.220772</td>\n      <td>-0.327231</td>\n      <td>-0.125243</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>27793</th>\n      <td>id_ffb710450</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.336446</td>\n      <td>-0.411697</td>\n      <td>-1.622411</td>\n      <td>-0.091706</td>\n      <td>-0.568255</td>\n      <td>-1.100391</td>\n      <td>-0.169549</td>\n      <td>0.531853</td>\n      <td>0.109870</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>27794</th>\n      <td>id_ffbb869f2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.292672</td>\n      <td>0.617046</td>\n      <td>0.026101</td>\n      <td>-1.747524</td>\n      <td>-1.242866</td>\n      <td>-0.207664</td>\n      <td>-0.742805</td>\n      <td>-0.091712</td>\n      <td>-0.628309</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>27795</th>\n      <td>id_ffd5800b6</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2</td>\n      <td>5</td>\n      <td>4</td>\n      <td>6</td>\n      <td>2</td>\n      <td>...</td>\n      <td>-1.542277</td>\n      <td>-0.071571</td>\n      <td>-0.236615</td>\n      <td>-0.937521</td>\n      <td>0.202393</td>\n      <td>0.283034</td>\n      <td>-1.284479</td>\n      <td>0.111258</td>\n      <td>-1.282890</td>\n      <td>test</td>\n    </tr>\n  </tbody>\n</table>\n<p>27796 rows × 2708 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n\nvar_thresh = VarianceThreshold(0.99)  \ndata_var_thresh = var_thresh.fit_transform(data.iloc[:, 4:-1])\n\nRemove_columns = np.array(data.columns[4:-1])[var_thresh.get_support()==False]\n\ntmp = pd.DataFrame(data_var_thresh, columns=np.array(data.columns[4:-1])[var_thresh.get_support()==True])\ndata = pd.concat([data.iloc[:,:4], tmp], axis=1)\n\nprint(f\"Remove {len(Remove_columns)} columns: {Remove_columns}\")","execution_count":42,"outputs":[{"output_type":"stream","text":"Remove 1331 columns: ['cnt_6' 'cnt_5' 'cnt_4' ... 'df_c-95' 'df_c-98' 'df_c-99']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['WHERE'] = Splitdata\ntrain = data[data['WHERE']==\"train\"].drop('WHERE', axis=1).reset_index(drop=True)\ntest = data[data['WHERE']==\"test\"].drop('WHERE', axis=1).reset_index(drop=True)","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Kolmogorov-Smirnov test applied for train data and test data.\n\nfrom scipy.stats import ks_2samp\n\ntr, ts = train.iloc[:, 4:], test.iloc[:, 4:]\nlist_p_value =[ks_2samp(ts[i], tr[i])[1] for i in tqdm(tr.columns)]\nSe = pd.Series(list_p_value, index=tr.columns).sort_values() \nlist_discarded = list(Se[Se < .1].index)\n\ntrain, test = train.drop(list_discarded, axis=1), test.drop(list_discarded, axis=1)\nprint(f\"Removed {len(list_discarded)} columns\")","execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=1372.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7811f176c00a4c268de188d16348ab16"}},"metadata":{}},{"output_type":"stream","text":"\nRemoved 337 columns\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = StratifiedKFold(n_splits=N_FOLD, shuffle=True, random_state=SEED)\n\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'learning_rate': Learning_rate,\n    'num_threads': 2,\n    'verbose': -1,\n    'max_depth': Max_depth,\n    'num_leaves': int((Max_depth**2)*0.7),\n    'feature_fraction':0.4, # randomly select part of features on each iteration\n    'lambda_l1':0.1,\n    'lambda_l2':0.1,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n}\n","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_importance_cols(feature_importance_df, num=10):\n    best_cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n            .groupby(\"Feature\")\n            .mean()\n            .sort_values(by=\"importance\", ascending=False)[:num].index)\n    return best_cols","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_featureimprotance(models, feature_importance_df):\n    for model in models:\n        _importance_df = pd.DataFrame()\n        _importance_df[\"Feature\"] = train.columns[1:]\n        _importance_df[\"importance\"] = model.feature_importance(importance_type='gain')\n        feature_importance_df = pd.concat([feature_importance_df, _importance_df], axis=0)\n        \n        return feature_importance_df","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = []\nfeature_importance_df = pd.DataFrame()\nimportance_cols_df = pd.DataFrame()\nscores = []\nmodels = []\n\nfor target_col in tqdm(train_targets_scored.columns[1:]):\n    _preds, _score, models, _feature_importance_df = run_lgbm(target_col)\n\n    sub[target_col] = _preds\n    scores.append(_score)\n    \n    if DEBUG:\n        if _score > 0.02:\n            importance_cols_df[target_col] = select_importance_cols(_feature_importance_df)\n            print(importance_cols_df)\n        \n        feature_importance_df = create_featureimprotance(models, feature_importance_df)","execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=206.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4f9ba3a466a476c941cd374de37fe51"}},"metadata":{}},{"output_type":"stream","text":"5-alpha_reductase_inhibitor , len(trt) : 17\n","name":"stderr"},{"output_type":"stream","text":"neg labels: 3289→ selected neg labels: 3286\n","name":"stdout"},{"output_type":"stream","text":"================= Pseudo labeling 1 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0137964\tvalid_1's binary_logloss: 0.0200054\n[200]\ttraining's binary_logloss: 0.00188047\tvalid_1's binary_logloss: 0.0050152\n[300]\ttraining's binary_logloss: 0.000628907\tvalid_1's binary_logloss: 0.00277838\n[400]\ttraining's binary_logloss: 0.000371247\tvalid_1's binary_logloss: 0.00216088\n[500]\ttraining's binary_logloss: 0.000324195\tvalid_1's binary_logloss: 0.00203734\nEarly stopping, best iteration is:\n[506]\ttraining's binary_logloss: 0.000321985\tvalid_1's binary_logloss: 0.00203191\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.013554\tvalid_1's binary_logloss: 0.025798\n[200]\ttraining's binary_logloss: 0.00179672\tvalid_1's binary_logloss: 0.00933456\n[300]\ttraining's binary_logloss: 0.000591815\tvalid_1's binary_logloss: 0.00704839\n","name":"stdout"},{"output_type":"stream","text":"1 / 3 AUC score:1.000\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[316]\ttraining's binary_logloss: 0.000526482\tvalid_1's binary_logloss: 0.00688588\n","name":"stdout"},{"output_type":"stream","text":"Threshold: 0.00496319561876093\nRemove_noisy_labels: 34 → positive_corect_labels: 328/3580\n30th percentile: 0.00013\np_label_rate: 0.01005 Vs.target_rate: 0.00093, Num_p_label: 40.0, conf_0:0.00013, conf_1:0.22533\nNum_p_label: 40.0, Expected: 3.7, Adj_threshold_1: 0.00\nNum_p_label: 3.0, Expected: 3.7, Adj_threshold_2: 0.04\nthreshold:0.04, positive p_label:3.0/3982, p_label_rate: 0.00075\npositive y_label:328.0/3580, y_label_rate: 0.09162\n================= Pseudo labeling 2 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0135295\tvalid_1's binary_logloss: 0.0197283\n[200]\ttraining's binary_logloss: 0.00177984\tvalid_1's binary_logloss: 0.00482491\n[300]\ttraining's binary_logloss: 0.000602411\tvalid_1's binary_logloss: 0.00247607\n[400]\ttraining's binary_logloss: 0.000360732\tvalid_1's binary_logloss: 0.00188044\nEarly stopping, best iteration is:\n[487]\ttraining's binary_logloss: 0.000321182\tvalid_1's binary_logloss: 0.00175865\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0133365\tvalid_1's binary_logloss: 0.0271424\n[200]\ttraining's binary_logloss: 0.00172719\tvalid_1's binary_logloss: 0.0107657\n","name":"stdout"},{"output_type":"stream","text":"2 / 3 AUC score:1.000\nThreshold: 0.04496319561876093\nRemove_noisy_labels: 2 → positive_corect_labels: 326.0/3578\n30th percentile: 0.00017\np_label_rate: 0.00126 Vs.target_rate: 0.00093, Num_p_label: 5.0, conf_0:0.00017, conf_1:0.26956\nthreshold:0.04, positive p_label:5.0/3982, p_label_rate: 0.00126\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[275]\ttraining's binary_logloss: 0.000709675\tvalid_1's binary_logloss: 0.00893694\n","name":"stdout"},{"output_type":"stream","text":"positive y_label:326.0/3578, y_label_rate: 0.09111\n================= Pseudo labeling 3 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0133468\tvalid_1's binary_logloss: 0.0236395\n[200]\ttraining's binary_logloss: 0.00177376\tvalid_1's binary_logloss: 0.00880718\n[300]\ttraining's binary_logloss: 0.000592723\tvalid_1's binary_logloss: 0.00542408\n[400]\ttraining's binary_logloss: 0.000354343\tvalid_1's binary_logloss: 0.0045398\nEarly stopping, best iteration is:\n[425]\ttraining's binary_logloss: 0.000333561\tvalid_1's binary_logloss: 0.00440707\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0131974\tvalid_1's binary_logloss: 0.0197874\n[200]\ttraining's binary_logloss: 0.00171763\tvalid_1's binary_logloss: 0.0045662\n[300]\ttraining's binary_logloss: 0.000576478\tvalid_1's binary_logloss: 0.00232478\n[400]\ttraining's binary_logloss: 0.000348699\tvalid_1's binary_logloss: 0.00179509\n","name":"stdout"},{"output_type":"stream","text":"3 / 3 AUC score:1.000\nThreshold: 0.04496319561876093\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[479]\ttraining's binary_logloss: 0.000313493\tvalid_1's binary_logloss: 0.00169189\n","name":"stdout"},{"output_type":"stream","text":"Remove_noisy_labels: 0 → positive_corect_labels: 326.0/3578\n30th percentile: 0.00009\np_label_rate: 0.00100 Vs.target_rate: 0.00093, Num_p_label: 4.0, conf_0:0.00009, conf_1:0.33207\nthreshold:0.04, positive p_label:4.0/3982, p_label_rate: 0.00100\npositive y_label:330.0/7560, y_label_rate: 0.04365\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x216 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","text":"len(train_index) : 3780\nlen(valid_index) : 3780\n================================= fold 1/2 5-alpha_reductase_inhibitor=================================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.00650208\tvalid_1's binary_logloss: 0.0194766\n","name":"stdout"},{"output_type":"stream","text":"len(train_index) : 3780\nlen(valid_index) : 3780\n================================= fold 2/2 5-alpha_reductase_inhibitor=================================\n","name":"stderr"},{"output_type":"stream","text":"Early stopping, best iteration is:\n[174]\ttraining's binary_logloss: 0.00130365\tvalid_1's binary_logloss: 0.0141923\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.00649594\tvalid_1's binary_logloss: 0.013207\n[200]\ttraining's binary_logloss: 0.000846613\tvalid_1's binary_logloss: 0.00603835\n[300]\ttraining's binary_logloss: 0.000284546\tvalid_1's binary_logloss: 0.00485966\n[400]\ttraining's binary_logloss: 0.000171014\tvalid_1's binary_logloss: 0.00445588\nEarly stopping, best iteration is:\n[428]\ttraining's binary_logloss: 0.000160213\tvalid_1's binary_logloss: 0.00443522\n","name":"stdout"},{"output_type":"stream","text":"5-alpha_reductase_inhibitor logloss: 0.00931373799853194\n=========================================================================================\n11-beta-hsd1_inhibitor , len(trt) : 18\n","name":"stderr"},{"output_type":"stream","text":"neg labels: 3289→ selected neg labels: 3286\n","name":"stdout"},{"output_type":"stream","text":"================= Pseudo labeling 1 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.016486\tvalid_1's binary_logloss: 0.0328989\n[200]\ttraining's binary_logloss: 0.00234636\tvalid_1's binary_logloss: 0.0136812\n[300]\ttraining's binary_logloss: 0.000772361\tvalid_1's binary_logloss: 0.00876114\n[400]\ttraining's binary_logloss: 0.000442819\tvalid_1's binary_logloss: 0.00731846\nEarly stopping, best iteration is:\n[469]\ttraining's binary_logloss: 0.000376914\tvalid_1's binary_logloss: 0.00681292\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0169427\tvalid_1's binary_logloss: 0.0291112\n[200]\ttraining's binary_logloss: 0.00237343\tvalid_1's binary_logloss: 0.00884414\n[300]\ttraining's binary_logloss: 0.00078023\tvalid_1's binary_logloss: 0.00496793\nEarly stopping, best iteration is:\n[350]\ttraining's binary_logloss: 0.000559572\tvalid_1's binary_logloss: 0.00434652\n","name":"stdout"},{"output_type":"stream","text":"1 / 3 AUC score:1.000\nThreshold: 0.005534050107441619\nRemove_noisy_labels: 34 → positive_corect_labels: 328/3580\n30th percentile: 0.00016\np_label_rate: 0.00954 Vs.target_rate: 0.00098, Num_p_label: 38.0, conf_0:0.00016, conf_1:0.01761\nNum_p_label: 38.0, Expected: 3.9, Adj_threshold_1: 0.01\nNum_p_label: 0.0, Expected: 3.9, Adj_threshold_2: 0.05\nthreshold:0.05, positive p_label:0.0/3982, p_label_rate: 0.00000\npositive y_label:328.0/3580, y_label_rate: 0.09162\n================= Pseudo labeling 2 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0162578\tvalid_1's binary_logloss: 0.0339723\n[200]\ttraining's binary_logloss: 0.00233214\tvalid_1's binary_logloss: 0.0148266\n[300]\ttraining's binary_logloss: 0.000763333\tvalid_1's binary_logloss: 0.00952041\n[400]\ttraining's binary_logloss: 0.000440462\tvalid_1's binary_logloss: 0.0080652\nEarly stopping, best iteration is:\n[466]\ttraining's binary_logloss: 0.000376257\tvalid_1's binary_logloss: 0.00767032\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0169676\tvalid_1's binary_logloss: 0.0264476\n[200]\ttraining's binary_logloss: 0.0023386\tvalid_1's binary_logloss: 0.00666358\n[300]\ttraining's binary_logloss: 0.000758311\tvalid_1's binary_logloss: 0.00383396\n","name":"stdout"},{"output_type":"stream","text":"2 / 3 AUC score:1.000\nThreshold: 0.04553405010744162\n","name":"stderr"},{"output_type":"stream","text":"[400]\ttraining's binary_logloss: 0.000436467\tvalid_1's binary_logloss: 0.00317419\nEarly stopping, best iteration is:\n[391]\ttraining's binary_logloss: 0.000451591\tvalid_1's binary_logloss: 0.0031726\n","name":"stdout"},{"output_type":"stream","text":"Remove_noisy_labels: 0 → positive_corect_labels: 328.0/3580\n30th percentile: 0.00014\np_label_rate: 0.00000 Vs.target_rate: 0.00098, Num_p_label: 0.0, conf_0:0.00014, conf_1:0.04553\nNum_p_label: 0.0, Expected: 3.9, Adj_threshold_1: 0.05\nthreshold:0.01, positive p_label:0.0/3982, p_label_rate: 0.00000\npositive y_label:328.0/3580, y_label_rate: 0.09162\n================= Pseudo labeling 3 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0162578\tvalid_1's binary_logloss: 0.0339723\n[200]\ttraining's binary_logloss: 0.00233214\tvalid_1's binary_logloss: 0.0148266\n[300]\ttraining's binary_logloss: 0.000763333\tvalid_1's binary_logloss: 0.00952041\n[400]\ttraining's binary_logloss: 0.000440462\tvalid_1's binary_logloss: 0.0080652\nEarly stopping, best iteration is:\n[466]\ttraining's binary_logloss: 0.000376257\tvalid_1's binary_logloss: 0.00767032\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0169676\tvalid_1's binary_logloss: 0.0264476\n[200]\ttraining's binary_logloss: 0.0023386\tvalid_1's binary_logloss: 0.00666358\n[300]\ttraining's binary_logloss: 0.000758311\tvalid_1's binary_logloss: 0.00383396\n","name":"stdout"},{"output_type":"stream","text":"3 / 3 AUC score:1.000\nThreshold: 0.005534050107441617\n","name":"stderr"},{"output_type":"stream","text":"[400]\ttraining's binary_logloss: 0.000436467\tvalid_1's binary_logloss: 0.00317419\nEarly stopping, best iteration is:\n[391]\ttraining's binary_logloss: 0.000451591\tvalid_1's binary_logloss: 0.0031726\n","name":"stdout"},{"output_type":"stream","text":"Remove_noisy_labels: 25 → positive_corect_labels: 328.0/3555\n30th percentile: 0.00014\np_label_rate: 0.00979 Vs.target_rate: 0.00098, Num_p_label: 39.0, conf_0:0.00014, conf_1:0.02079\nNum_p_label: 39.0, Expected: 3.9, Adj_threshold_1: 0.01\nNum_p_label: 0.0, Expected: 3.9, Adj_threshold_2: 0.05\nthreshold:0.05, positive p_label:0.0/3982, p_label_rate: 0.00000\npositive y_label:328.0/7537, y_label_rate: 0.04352\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x216 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"stream","text":"len(train_index) : 3768\nlen(valid_index) : 3769\n================================= fold 1/2 11-beta-hsd1_inhibitor=================================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.00802311\tvalid_1's binary_logloss: 0.0173304\n[200]\ttraining's binary_logloss: 0.00112577\tvalid_1's binary_logloss: 0.006621\n[300]\ttraining's binary_logloss: 0.000359068\tvalid_1's binary_logloss: 0.00402969\n[400]\ttraining's binary_logloss: 0.000201909\tvalid_1's binary_logloss: 0.00322226\nEarly stopping, best iteration is:\n[475]\ttraining's binary_logloss: 0.000173104\tvalid_1's binary_logloss: 0.00298986\n","name":"stdout"},{"output_type":"stream","text":"len(train_index) : 3769\nlen(valid_index) : 3768\n================================= fold 2/2 11-beta-hsd1_inhibitor=================================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.00825673\tvalid_1's binary_logloss: 0.0144351\n[200]\ttraining's binary_logloss: 0.00115777\tvalid_1's binary_logloss: 0.00398881\n[300]\ttraining's binary_logloss: 0.000363046\tvalid_1's binary_logloss: 0.00230196\n[400]\ttraining's binary_logloss: 0.000205271\tvalid_1's binary_logloss: 0.00190739\nEarly stopping, best iteration is:\n[407]\ttraining's binary_logloss: 0.000200264\tvalid_1's binary_logloss: 0.00187697\n","name":"stdout"},{"output_type":"stream","text":"11-beta-hsd1_inhibitor logloss: 0.002433486702319096\n=========================================================================================\nacat_inhibitor , len(trt) : 24\n","name":"stderr"},{"output_type":"stream","text":"neg labels: 3289→ selected neg labels: 3285\n","name":"stdout"},{"output_type":"stream","text":"================= Pseudo labeling 1 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0223951\tvalid_1's binary_logloss: 0.0394519\n[200]\ttraining's binary_logloss: 0.0034043\tvalid_1's binary_logloss: 0.0125851\n[300]\ttraining's binary_logloss: 0.00108212\tvalid_1's binary_logloss: 0.00694623\n[400]\ttraining's binary_logloss: 0.000581742\tvalid_1's binary_logloss: 0.00513725\nEarly stopping, best iteration is:\n[429]\ttraining's binary_logloss: 0.000517809\tvalid_1's binary_logloss: 0.00494294\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0212541\tvalid_1's binary_logloss: 0.0439502\n[200]\ttraining's binary_logloss: 0.00349295\tvalid_1's binary_logloss: 0.0134978\n[300]\ttraining's binary_logloss: 0.00108601\tvalid_1's binary_logloss: 0.00731831\n[400]\ttraining's binary_logloss: 0.000575017\tvalid_1's binary_logloss: 0.00535554\nEarly stopping, best iteration is:\n[446]\ttraining's binary_logloss: 0.000488132\tvalid_1's binary_logloss: 0.00513739\n","name":"stdout"},{"output_type":"stream","text":"1 / 3 AUC score:1.000\nThreshold: 0.004472865870108365\nRemove_noisy_labels: 34 → positive_corect_labels: 328/3579\n30th percentile: 0.00022\np_label_rate: 0.00703 Vs.target_rate: 0.00125, Num_p_label: 28.0, conf_0:0.00022, conf_1:0.01315\nNum_p_label: 28.0, Expected: 5.0, Adj_threshold_1: 0.00\nNum_p_label: 0.0, Expected: 5.0, Adj_threshold_2: 0.04\nthreshold:0.04, positive p_label:0.0/3982, p_label_rate: 0.00000\npositive y_label:328.0/3579, y_label_rate: 0.09165\n================= Pseudo labeling 2 / 3 =================\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0215953\tvalid_1's binary_logloss: 0.0377117\n[200]\ttraining's binary_logloss: 0.00335666\tvalid_1's binary_logloss: 0.0120876\n[300]\ttraining's binary_logloss: 0.00107366\tvalid_1's binary_logloss: 0.00656457\n[400]\ttraining's binary_logloss: 0.000577811\tvalid_1's binary_logloss: 0.0048008\n[500]\ttraining's binary_logloss: 0.000439297\tvalid_1's binary_logloss: 0.00437406\nEarly stopping, best iteration is:\n[490]\ttraining's binary_logloss: 0.000447141\tvalid_1's binary_logloss: 0.00433562\nTraining until validation scores don't improve for 10 rounds\n[100]\ttraining's binary_logloss: 0.0206349\tvalid_1's binary_logloss: 0.042581\n\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-66c40746d1da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtarget_col\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_targets_scored\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0m_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_feature_importance_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_lgbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-33a95032e37f>\u001b[0m in \u001b[0;36mrun_lgbm\u001b[0;34m(target_col)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sig_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_importance_df_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpseudo_labeling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-49-17403afb5612>\u001b[0m in \u001b[0;36mpseudo_labeling\u001b[0;34m(X_train, y_train, X_test, target_rate, target_col, max_iter)\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'logloss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     early_stopping_rounds=Early_stopping_rounds)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0my_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    803\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                                         callbacks=callbacks)\n\u001b[0m\u001b[1;32m    806\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    598\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1974\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1975\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1976\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1977\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1978\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"CV:{np.mean(scores)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DEBUG:\n    show_feature_importance(feature_importance_df)\n    feature_importance_df.to_csv(f'feature_importance_df.{Version}.csv', index=False)\n    importance_cols_df.to_csv(f'importance_cols_df.{Version}.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}